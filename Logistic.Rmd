# Introduction. {.unnumbered}

Credit risk models play a pivotal role in the financial landscape, providing institutions with a systematic framework to assess and manage the risks associated with lending and investment activities. One crucial component of these models is the estimation of the probability of default, which quantifies the likelihood that a borrower will fail to meet their debt obligations. The relevance of credit risk models lies in their ability to enhance decision-making processes by offering a comprehensive understanding of the potential creditworthiness of individuals, companies, or financial instruments. By utilizing statistical methods, historical data, and various financial indicators, these models enable financial institutions to make informed decisions about lending, pricing, and portfolio management. The estimation of the probability of default not only aids in risk assessment but also supports regulatory compliance, allowing institutions to maintain a healthy balance between risk and return in their credit portfolios. In an ever-evolving financial landscape, the continual refinement and application of credit risk models are essential for fostering stability and resilience within the financial system.

# Loan analysis.

The main objective of this section is to predict whether a person or client applying for a loan will repay it or not. This issue can be considered a classification problem, where based on information about the client and the characteristics of their loan application, the client is classified into one of two categories: whether they will default or not. There are various ways to approach this classification problem; in this section, we will use a simple machine learning model called logistic regression. The model is first estimated or trained and then evaluated using new data to determine how well it performs the classification.

```{r echo=FALSE}
# This removes all items in environment. 
# It is a good practice to start your code this way.
rm(list=ls())
```

Let's load the required R packages first.

```{r Loan analysis}
# Logistic models
library(gmodels) # CrossTable()
library(ggplot2)
library(tidyr) # gather()
library(dplyr)
library(pROC) # roc
library(vembedr)
```

This section relies on the DataCamp course *Credit Risk Modeling in R* by Lore Dirick. However, we incorporate a slightly different database and an extended analysis.

## Explore the database.

Let's load the data called <tt>`loan_data_ARF.rds`</tt> and then understand its structure before conducting any further analysis. This database is available [here](https://github.com/mlozanoqf/tutorial_arf/blob/main/loan_data_ARF.rds).

```{r Create database, eval=FALSE, include=FALSE}
dat <- readRDS("loan_data_ch1.rds")
# Add sex variable.
# Sex = 1 female; sex = 0 male.
set.seed(1)
sex_1 <- rbinom(n = nrow(dat[dat$loan_st == 1, ]), 
                size = 1, prob = 0.3) # Let's make females default less.
sex_0 <- rbinom(n = nrow(dat[dat$loan_st == 0, ]), 
                size = 1, prob = 0.55) # Let's make females no default more.
sex <- ifelse(dat$loan_st == 1, sex_1, sex_0)
dat$sex <- sex
dat$sex <- as.factor(dat$sex)

# Add region variable.
set.seed(1)
row_def <- nrow(dat[dat$loan_st == 1, ])
row_nodef <- nrow(dat[dat$loan_st == 0, ])

def_region_index <- sample(c("N", "E", "W", "S"), size = row_def, 
                           replace  = TRUE, prob = c(0.1, 0.2, 0.3, 0.4))
nodef_region_index <- sample(c("N", "E", "W", "S"), size = row_nodef, 
                           replace  = TRUE, prob = c(0.4, 0.3, 0.2, 0.1))

region <- ifelse(dat$loan_st == 1, 
                 def_region_index, nodef_region_index)
dat$region <- region
dat$region <- as.factor(dat$region)

# Imputation.
index_NA <- which(is.na(dat$emp_length))
dat$emp_length[index_NA] <- median(dat$emp_length, na.rm = TRUE)

index_NA <- which(is.na(dat$int_rate))
dat$int_rate[index_NA] <- median(dat$int_rate, na.rm = TRUE)

# New database
saveRDS(dat, "loan_data_ARF.rds")
```

```{r Load the modified database}
dat <- readRDS("loan_data_ARF.rds")
str(dat)
```


This dataset could represent a typical collection of data from a financial institution, such as a bank or a company that uses credit channels to sell its products or services. It contains 29,092 observations across 10 variables. Each observation corresponds to the personal and loan characteristics of an individual loan. 

A key variable, our dependent variable, is `loan_st`, which indicates loan status. A value of 0 represents no default, while a value of 1 represents default. A default occurs when a borrower fails to make timely payments, misses payments, or ceases payments on the interest or principal owed. The definition of default can vary depending on the goals and interests of the analysis. For the purposes of this study, we classify loans simply as either default or no default. 

The variable `loan_st` is dichotomous, or categorical. Our primary interest lies in predicting whether a new loan application will result in default or not.

Clearly, `loan_data_ARF.rds` represents past information, as we know with certainty whether an individual defaulted (1) or not (0). Historical data is valuable for understanding how likely an individual is to default based on their personal and loan characteristics. It is also essential for training quantitative models to predict outcomes for new applicants and for evaluating the performance of our classifications.

A variable name is considered too long when a shorter name can convey the same purpose just as effectively. Let's rename some of them.

```{r}
old_names <- colnames(dat)
colnames(dat) <- c("loan_st", "l_amnt", "int", "grade", "emp_len", 
                   "home", "income", "age", "sex", "region")
data.frame(old_names, "new_names" = colnames(dat))
```

New variable names have now been assigned. 

We can take a look at the information in different ways. For example, look at the first 10 rows out of 29,092.

```{r First 10 rows of loan data}
head(dat, 10)
```

Note that <tt>`sex`</tt> is 1 for female and 0 for male. Now, instead of looking the details of the first 10 rows, we can summarize with respect to <tt>`home`</tt> with the <tt>`CrossTable()`</tt> function.

```{r CrossTable as a way to summarize data}
CrossTable(dat$home)
```

We can also use <tt>`CrossTable()`</tt> to summarize two variables. In particular, instead of counting for home ownership we can add a second dimension like <tt>`loan_st`</tt>. This allows us to create more informative tables.

```{r CrossTable using two dimensions}
CrossTable(dat$home, dat$loan_st, prop.r = TRUE,
           prop.c = FALSE, prop.t = FALSE, prop.chisq = FALSE)
```

This table reveals defaults by home ownership. We can use histograms to see one variable distribution. In this case we have the interest rate distribution.

```{r}
#| label: fig-irh
#| fig-cap: "Interest rate histogram."
ggplot(dat, aes(x = int)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.5, colour = "black", 
                 fill = "white") +
  labs(y = "Density", x = "Interest rate") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

The following is a dotplot figure for the annual income.

```{r}
#| label: fig-aib
#| fig-cap: "Annual income dotplot."
ggplot(dat, aes(income)) + 
  geom_dotplot(binwidth = 100000, fill = "blue") +
  labs(y = "Density",
       x = "Annual income") +
  theme(legend.position = "bottom", legend.title = element_blank())
```
The @fig-aib above appears suspicious. The horizontal axis shows a very large annual income value (6,000,000). Additionally, there are only a few individuals with extremely high incomes. We should investigate further to determine whether these are valid observations or errors in the original dataset.

```{r Extract the row of the high income individuals}
high_income <- dat[(dat$income > 1000000), ]
high_income
```

One individual (ID 19486) is not only extremely wealthy but also 144 years old. As a result, I have decided to remove these nine observations. Data cleaning is a common task when working with large datasets, and it is acceptable as long as the integrity of the data remains intact.

```{r}
high_income_index <- data.frame(value = as.integer(rownames(high_income)))
dat <- dat[-high_income_index$value,]
```

Remember, the original dataset contains 29,092 rows. After removing 9 observations, we are left with 29,083 rows. Let's take a look at the result.

```{r}
#| label: fig-aiwev
#| fig-cap: "Annual income without extreme values."
ggplot(dat, aes(income)) + 
  geom_dotplot(binwidth = 10000, fill = "blue") +
  labs(y = "Density", x = "Annual income") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

Somewhat better now.

We could also explore some gender issues.

```{r Gender issues}
summary_table <- dat |>
  group_by(sex) |>
  summarize(
    Mean = mean(income),
    Median = median(income),
    Min = min(income),
    Max = max(income),
    Count = n()
  ) |>
  mutate(
    Gender = case_when(
      sex == 0 ~ "Male",
      sex == 1 ~ "Female"
    )
 )
  
summary_table
```

The table shows that females tend to have slightly higher average and median incomes compared to males. Additionally, the income distribution for females appears more variable, with a wider range of incomes. While the sample sizes for both groups are similar, females are slightly more represented. Overall, the data suggests a modest income advantage for females and greater variability in their earnings.

The better we understand the data, the better positioned we are to analyze it, gain deeper insights into the problem, and interpret the solutions more effectively. However, there is a risk of falling into the trap of overexploration, losing sight of the main problem and failing to address the objectives. Data cleaning, for instance, can be an ongoing task. Removing outliers with excessively high incomes is just one example of how data cleaning can be approached. For now, we pause the exploration and cleaning process and move on to the next section, where we estimate models that will help us achieve the outlined objectives.

## Logistic models.

Logistic models allows us to make predictions about loan defaults. Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable like <tt>`loan_st`</tt>. In this case, the binary dependent variable is default (1) or no default (0). A good reference for this section is @hull2020machine.

First, load the data and split it into two sets: (1) training and (2) test. The training set is for building and estimate models, and the test set is used to evaluate our model predictions with new data. When estimating models, it is common practice to separate the available data into two parts, *training* and *test* data, where the training data is used to estimate parameters (in-sample) and the test data is used to evaluate its accuracy (out of sample). Because the test data is not used in determining the estimation, it should provide a reliable indication of how well the model is likely to estimate or forecast on new data. In sum, we train the model, we test the model, and once we are OK with the model performance on new data, we are ready to use it in real-life applications. If we ignore this split and use the whole database to estimate our models, we may succeed at explaining defaults in our database but we may fail to explain defaults for new loan applications.

```{r Training and test set construction}
# It is convenient to set the loan status as factor.
dat$loan_st <- as.factor(dat$loan_st)
set.seed(567)
index_train <- cbind(runif(1 : nrow(dat), 0 , 1), c(1 : nrow(dat)))
index_train <- order(index_train[, 1])
index_train <- index_train[1: (2/3 * nrow(dat))]
# Create training set
train <- dat[index_train, ]
# Create test set
test <- dat[-index_train, ]
```

Variables as factors are useful for model estimation and data visualization. Factors are variables in R which take on a limited number of different values; such variables are often referred to as categorical variables.

We have 29,083 observations in <tt>`dat`</tt>. The code above randomly selects $29083 \times (2/3)=19388$ rows to form the <tt>`train`</tt>. The <tt>`test`</tt> are the remaining $29083-19388=9695$ rows. The random selection is highly recommended as the <tt>`dat`</tt> may have some structure or sorting that could bias our model estimation and negatively impact our model test. For example, imagine that for some weird reason the database is sorted in such a way that the first observations are all no default. If that is the case, then the training and the test set would not have portions of default and no default cases and we may distort the whole analysis. The random selection allows us to replicate a real situation in which our database is unsorted, with different characteristics.

Let's see if the recent created <tt>`train`</tt> and <tt>`test`</tt> have the same basic properties than <tt>`dat`</tt>.

```{r}
# Combine data into a list for processing
data_list <- list(dat = dat$loan_st, train = train$loan_st, test = test$loan_st)

# Compute proportions and combine into a data frame
prop <- do.call(rbind, lapply(data_list, function(x) prop.table(table(x))))
colnames(prop) <- c("no defaults", "defaults")
row.names(prop) <- c("dat_prop", "train_prop", "test_prop")
prop
```

The proportions of "defaults" and "no defaults" in the train and test datasets are very similar to those in the original dataset, indicating that both samples are representative. Specifically, the proportion of "defaults" in the train and test datasets closely matches that of the original dataset, as does the proportion of "no defaults." This suggests that the random sampling process has preserved the overall distribution of the target variable, ensuring that the train and test sets reflect the characteristics of the full dataset.

Assume we think that the <tt>`loan_st`</tt> depends on the age of the individual. We can estimate a simple logistic model to learn about the relationship between age and loan status.


$\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 \times \text{age}$.

Where:

* $p$ is the probability that `loan_st = 1` (e.g., the probability of default).

* $\beta_0$ is the intercept, and $\beta_1$ is the coefficient for the predictor variable `age`.

The dependent variable in logistic regression is expressed as: $\log\left(\frac{p}{1-p}\right)$. This transformation, known as the *logit function*, maps probabilities $p$ which range between 0 and 1 to the entire real number line $-\infty$ to $+\infty$. This allows the model to establish a linear relationship between the independent variable(s) (e.g., age) and the transformed dependent variable, making estimation feasible. The fraction $\frac{p}{1-p}$, called the *odds*, represents the ratio of the probability of an event happening $p$ to the probability of it not happening $1-p$, making it a natural choice for modeling binary outcomes.

Let's estimate the age model.

```{r logi_age estimation}
# Fitting a simple logistic model.
logi_age <- glm(loan_st ~ age, family = "binomial", data = train)
logi_age
```

The estimated model is: $\log\left(\frac{p}{1-p}\right) = -1.90097 - 0.00623 \times \text{age}$.

The model suggests there is a negative relationship between age and loan status. 

We can solve for $p$ to find the estimate of the probability of default according to the age model: $p = \frac{\exp(-1.90097 - 0.00623 \times \text{age})}{1 + \exp(-1.90097 - 0.00623 \times \text{age})}$. Let's evaluate $p$ for a couple of age values.

Substituting `age = 18`: $p = \frac{\exp(-1.90097 - 0.00623 \times 18)}{1 + \exp(-1.90097 - 0.00623 \times 18)}$,

$p = \frac{\exp(-2.01311)}{1 + \exp(-2.01311)} \approx \frac{0.1333}{1 + 0.1333} \approx 0.1176$.

Substituting `age = 60`: $p = \frac{\exp(-1.90097 - 0.00623 \times 60)}{1 + \exp(-1.90097 - 0.00623 \times 60)}$,

$p = \frac{\exp(-2.27477)}{1 + \exp(-2.27477)} \approx \frac{0.1037}{1 + 0.1037} \approx 0.0939$.

The results of the default probability estimation evaluated at ages 18 and 60 are not very promising because the difference in probabilities is very small. This means that the model does not generate significantly differentiated default probabilities, even when evaluated over a wide age range. This can be problematic because it suggests that this is a poor model.

The following is a graphical representation of the logistic regression model's output.

```{r}
#| label: fig-scolcam
#| fig-cap: "Sigmoid curve or logistic curve: age model."
# Define the function for the logistic regression model
logistic_function <- function(age) {
  beta0 <- -1.90097
  beta1 <- -0.00623
  p <- exp(beta0 + beta1 * age) / (1 + exp(beta0 + beta1 * age))
  return(p)
}

# Generate a sequence of ages
ages <- seq(-1000, 500, by = 1)

# Apply the logistic function to each age
p_values <- sapply(ages, logistic_function)

# Create the plot
library(ggplot2)
ggplot(data = data.frame(age = ages, p = p_values), 
       aes(x = age, y = p)) +
  geom_line(color = "blue", size = 2) +
  labs(x = "Age", y = "Probability (p)") +
  theme_minimal()
```
The logistic curve derived from this model is unreasonable because the probability $p$ changes very slowly with respect to age. This implies that, to observe the full range of probabilities from 0 to 1, we would require age values that are unrealistic or implausibly extreme. Such behavior suggests that the model is poorly specified, as it fails to generate meaningful distinctions in probabilities across a realistic range of ages.

The AIC value of the age model (13,580) is useful when comparing models. The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data. At the moment we cannot interpret the AIC simply because we only have one model and we cannot compare it with another AIC.

Let's estimate another simple model where the interest rate category is used as a predictor of the <tt>`loan_st`</tt>. Remember we are not conducting any prediction at all at this moment, we are only estimating models using the training set.

```{r logi_int estimation}
# Build a glm model with variable interest rate as a predictor.
logi_int <- glm(formula = loan_st ~ int, family = "binomial", data = train)
# Print the parameter estimates.
logi_int
```

The AIC is a lower (13,220 versus 13,580), so we have a better model now.

Using one single predictor as age or interest rate is clearly a limited approach. Let's add some more predictors. Also, let's introduce the <tt>`summary()`</tt> function to extract more information about the model estimation results. The <tt>`logi_multi`</tt> below assumes that the loan status depend on the age, interest rate, grade, loan amount, and annual income.

```{r logi_multi estimation}
# Multiple variables in a logistic regression model.
logi_multi <- glm(loan_st ~ age + int + grade + log(l_amnt) + 
                  log(income) , family = "binomial", data = train)
# Obtain significance levels using summary().
summary(logi_multi)
```

Our multi-factor model works well. In <tt>`logi_multi`</tt>, the AIC value is the lowest so far (13,050 versus 13,220), so this should be considered as the best in-sample model at the moment. The <tt>`summary()`</tt> function shows the significance levels of the estimators, but we are currently more interested in the goodness of fit of the models because we want to conduct predictions about the <tt>`loan_st`</tt>. This is, we are interested to use a model to find out whether new applicants in the test set are expected to default or not, rather than in the applicants' credit risk factors. This is why we are concentrated in AIC now.

When a customer fill out a credit application form, we collect information but we do not know for sure whether she or he will eventually default. A credit risk model can help us in this task.

## Prediction and model evaluation.

Let's evaluate the predictive performance of our three models: `logi_age`, `logi_int`, and `logi_multi`, introduced in the previous subsection. We begin by selecting a single observation from the test set and asking each model to predict the `loan_st` (loan status) for this individual. Specifically, we take the age of the first person in the test set, use it as input for the `logi_age` model, and then compare the model's predicted loan_st with the actual outcome recorded in the test set. Since the test set contains the true loan status for this individual, we can directly assess how well the model performed. Each model is expected to generate a potentially different prediction for the `loan_st`. A good model should produce a prediction that matches the actual outcome for this observation, indicating its reliability in capturing the underlying patterns in the data.

Let's call the first individual John Doe for practical purposes.

```{r One applicant}
# Define one single observation in test_set.
John_Doe <- as.data.frame(test[1, ])
John_Doe
```

We know in advance that the `loan_st` for this observation, taken from the test set, is 0. However, the models cannot know this because the test set was not used to estimate the logistic models. Instead, the models were trained using the training set. A good credit risk model should predict no default for this new applicant called John Doe.

The values of `loan_st` in the test set are either 0 or 1. However, the logistic models predict the `loan_st` as probability estimates within the range of 0 to 1, as illustrated in @fig-scolcam. This means we would expect the estimated `loan_st` for John Doe to be close to 0. *But how close?* We will address this issue later.

This code predicts the loan status for John Doe using three logistic regression models. Each prediction represents the probability that John Doe defaults on a loan, based on the respective model.

```{r Predictions of one single applicant in the test set}
# List of models
models <- list("logi_age" = logi_age, "logi_int" = logi_int, "logi_multi" = logi_multi)

# Predict the loan status for John Doe using all models
pred_John <- sapply(models, predict, newdata = John_Doe, type = "response")

# Convert to a table with appropriate column name
pred_John <- as.data.frame(pred_John)
colnames(pred_John) <- "Loan status predictions for John Doe."

# Display the predictions
pred_John
```

These values are low, as they are close to 0. We could interpret this as some ability of the models to correctly predict that John Doe will not default. However, we have already illustrated that at least the age model is highly problematic. Additionally, several questions remain unanswered and require further analysis. For example, *how can we determine if the prediction is low enough to classify it as a non-default?* We may need a cutoff value to make this decision. We will explore this issue later.

Another important consideration is: *What about the rest of the cases in the test set?* The test set contains 9,695 observations, yet the example above only evaluates the first one. Our interest lies in assessing the entire test set, not just John Doe. Fortunately, this issue is easy to resolve by modifying the `newdata` parameter in the `predict()` function. Specifically, instead of using `newdata = John_Doe`, which refers to a single observation, we can replace it with `newdata = test`, encompassing all 9,695 observations in the test set.

<!-- ```{r Predictions of the entire test set} -->
<!-- # List of models -->
<!-- models <- list("logi_age" = logi_age, "logi_int" = logi_int, "logi_multi" = logi_multi) -->

<!-- # Predict loan status and calculate range and AIC for each model -->
<!-- pred_range <- sapply(models, function(model) { -->
<!--   c(range(predict(model, newdata = test, type = "response")), AIC(model)) -->
<!-- }) -->

<!-- # Convert to a data frame and label the columns -->
<!-- pred_range <- as.data.frame(t(pred_range)) -->
<!-- colnames(pred_range) <- c("min(loan_st)", "max(loan_st)", "AIC") -->

<!-- # Display the result -->
<!-- pred_range -->
<!-- ``` -->


```{r Predictions of the entire test set}
# Predict the loan status with the three models.
pred_logi_age <- predict(logi_age, newdata = test, type = "response")
pred_logi_int <- predict(logi_int, newdata = test, type = "response")
pred_logi_multi <- predict(logi_multi, newdata = test,
                           type = "response")

pred_range <- rbind("logi_age" = range(pred_logi_age),
                     "logi_int" = range(pred_logi_int),
                     "logi_multi" = range(pred_logi_multi))
aic <- rbind(logi_age$aic, logi_int$aic, logi_multi$aic)
pred_range <- cbind(pred_range, aic)
colnames(pred_range) <- c("min(loan_st)", "max(loan_st)", "AIC")
pred_range
```

Now, instead of predicting for a single applicant, we have made predictions for all 9,695 applicants in the test set. The first column shows the lower predicted `loan_st` for each model, and the logistic models produce values in the range of 0 to 1. In this case, the predicted ranges are quite narrow.

Narrow ranges (i.e., the small difference between the highest and lowest predicted `loan_st` values) could be problematic because the model might struggle to discriminate between defaults (predictions closer to 1) and non-defaults (predictions closer to 0) as suggested in @fig-scolcam. The higher the AIC, the worse the in-sample model, and the lower the AIC, the better the model. Here, we see some consistency between in-sample and out-of-sample performance: the model with the highest prediction range also has the lowest AIC, indicating it is the best in-sample model.

Let's explore all the predicted <tt>`loan_st`</tt> values for the <tt>`logi_age`</tt>. First let's see all of them.

```{r}
#| label: fig-amp
#| fig-cap: "Age model predictions."
# Convert the predictions to a data frame
pred_data <- data.frame(
  Observation = seq_along(pred_logi_age),  # Sequence for observation index
  Predicted_Probabilities = pred_logi_age
)

# Create the plot
ggplot(pred_data, aes(x = Observation, y = Predicted_Probabilities)) +
  geom_point(alpha = 0.6, color = "blue") +  # Points for predictions
  labs(x = "Observation (test set)", 
       y = "Default prediction") +
  theme_minimal()
```
Now its distribution.

```{r}
#| label: fig-amph
#| fig-cap: "Age model prediction histogram."
ggplot(data.frame(pred_logi_age), aes(x = pred_logi_age)) + 
  geom_density(fill = "red") +
  labs(y = "Density", x = "Default prediction") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

The <tt>`logi_age`</tt> fails to predict values ranging from 0 to 1. In fact, these values are quite concentrated in a very small range of values. As a consequence, this model fails to differentiate between default and no default predictions.

Let's visualize the predictions of the <tt>`logi_int`</tt> and <tt>`logi_multi`</tt>. We first collect all predictions in a single data frame just for convenience.

```{r}
pred_logi <- data.frame(cbind(pred_logi_age, pred_logi_int,
                                  pred_logi_multi))
pred_logi <- gather(pred_logi, key = "model",  value = "pred")
```

Now we plot the <tt>`logi_int`</tt> and <tt>`logi_multi`</tt>.

```{r}
#| label: fig-irammph
#| fig-cap: "Interest rate and multi models predictions histograms."
ggplot(pred_logi[pred_logi$model != "pred_logi_age",], 
       aes(x = pred, fill = model)) + 
  geom_density(alpha = 0.4) +
  labs(y = "Density", x = "Default prediction") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

Let's add the <tt>`age`</tt> model as well.

```{r}
#| label: fig-aairaammpb
#| fig-cap: "Age, Interest rate and multi models predictions boxplot."
ggplot(pred_logi, aes(x = pred, y = model, fill = model)) + 
  geom_boxplot() +
  labs(y = "Model", x = "Default prediction") +
  theme(legend.position = "none", legend.title = element_blank())
```

Presumably, a model which considers all available predictors could be better for predicting the <tt>`loan_st`</tt>.

```{r logi_full estimation and prediction}
# Logistic regression model using all available predictors in the data set.
logi_full <- glm(loan_st ~ age + int + grade + log(l_amnt) + 
                   log(income) + emp_len + home + sex +
                   region, family = "binomial", data = train)
# Loan status predictions for all test set elements.
pred_logi_full <- predict(logi_full, newdata = test, type = "response")
# Look at the predictions range.
range(pred_logi_full)
```

Now, the <tt>`pred_logi_full`</tt> prediction range is wider. A wider range means that the <tt>`loan_st`</tt> predictions are now closer to 1. This is good because we need the model to be able to predict both no-defaults (0) and defaults (1). Let's see a prediction comparison with respect to the rest of the models.

```{r}
pred_range <- rbind("logi_age" = range(pred_logi_age), 
                     "logi_int" = range(pred_logi_int),
                     "logi_multi" = range(pred_logi_multi),
                     "logi_full" = range(pred_logi_full))
aic <- rbind(logi_age$aic, logi_int$aic, logi_multi$aic, logi_full$aic)
pred_range <- cbind(pred_range, aic)
colnames(pred_range) <- c("min(loan_st)", "max(loan_st)", "AIC")
pred_range
```

```{r}
pred_logi <- data.frame(cbind(pred_logi_age, pred_logi_int,
                                  pred_logi_multi, pred_logi_full))
pred_logi <- gather(pred_logi, key = "model",  value = "pred")
```

A graphical comparison of the new <tt>`pred_logi_full`</tt> and <tt>`pred_logi_multi`</tt>.

```{r}
#| label: fig-mafmph
#| fig-cap: "Multi and full models predictions histograms."
ggplot(pred_logi[pred_logi$model != "pred_logi_age" &
                  pred_logi$model != "pred_logi_int" ,], 
       aes(x = pred, fill = model)) + 
  geom_density(alpha = 0.4) +
  labs(y = "Density", x = "Default prediction") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

And all of them together.

```{r}
#| label: fig-airmafmpb
#| fig-cap: "Age, Interest rate, multi and full models predictions boxplot."
ggplot(pred_logi, aes(x = pred, y = model, fill = model)) + 
  geom_boxplot() +
  labs(y = "Model", x = "Default prediction") +
  theme(legend.position = "none", legend.title = element_blank())
```

The <tt>`logi_full`</tt> model predictions looks better than the other models.

Another question is: *How can we know these model predictions corresponds to a default or no default?* The loan status predictions go from 0 to 0.854 for the case of the <tt>`logi_full`</tt> model. At the end, these loan status estimations need to be interpreted or classified as a default or no default because we are interested on that. *Are they closer enough to 0 to consider a no default?* This issue is addressed by setting up a cutoff rate so we can split all estimated loan status into 0 or 1.

Let us arbitrarily set a cutoff of 0.15 for now. This implies that any estimated loan status below 0.15 will be classified as 0 (no default), while any estimated loan status above 0.15 will be classified as 1 (default). This classification rule can guide financial firms in deciding whether to approve or reject new loan applications. Applications with an estimated loan status above 0.15 will be rejected, whereas those with an estimated loan status below 0.15 will be approved. Thus, the loan approval or rejection decision is now based on the model's estimation.

Choosing a cutoff value as a classification and acceptance rule can be controversial, as managers may prefer to allocate more loans rather than fewer. In other words, managers may prefer more sales at the cost of a higher credit risk. Specifically, the higher the cutoff, the greater the number of loans allocated, since the allocation rule selects those applicants whose predicted loan status is below the cutoff.

An alternative that balances credit risk with managerial interests is to implement multiple cutoff ranges instead of relying on a single cutoff value, such as 0.15. For instance, a financial firm could accept loan applications with an estimated loan status between 0.15 and 0.25 but charge a higher interest rate to offset the increased credit risk. Alternatively, a different perspective might involve incorporating social criteria to support individuals typically excluded by the financial industry due to their economic circumstances. In such cases, the focus could shift toward identifying applicants with lower estimated loan statuses.

For now, our cutoff and allocation rule assume that any estimated loan status below 0.15 will be classified as 0 (no default), while any estimated loan status above 0.15 will be classified as 1 (default).

Graphically, it looks like this:

```{r}
#| label: fig-fmpaco
#| fig-cap: "Full model predictions and cutoff of 0.15."
# Convert the predictions to a data frame
pred_data <- data.frame(
  Observation = seq_along(pred_logi_full),  # Sequence for observation index
  Predicted_Probabilities = pred_logi_full
)

# Create the plot
ggplot(pred_data, aes(x = Observation, y = Predicted_Probabilities)) +
  geom_point(alpha = 0.6, color = "blue") +  # Points for predictions
  geom_hline(yintercept = 0.15, color = "red", size = 1) +  # Horizontal line
  labs(x = "Observation (test set)", 
       y = "Default prediction") +
  theme_minimal()

```
Alternatively, we can illustrate the distribution.

```{r}
#| label: fig-fmphaco
#| fig-cap: "Full model predictions histogram and cutoff of 0.15."
ggplot(pred_logi[pred_logi$model == "pred_logi_full",], 
       aes(x = pred, fill = model)) + 
  geom_density(alpha = 0.4) +
  geom_vline(xintercept = 0.15, linetype = "longdash") +
  labs(y = "Density", x = "Default prediction") +
  theme(legend.position = "none", legend.title = element_blank())
```

Here, predicted <tt>`loan_st`</tt> values at the left of the dashed line represent no default predictions and values at the right of the dashed line represent default predictions. Let's set up the rule to convert the estimated loan status into a binary (0 or 1) variable. 

See how this transformation takes place:

```{r Specifying a cutoff of 15}
# Make a binary predictions-vector using a cutoff of 15%
pred_cutoff_15 <- ifelse(pred_logi_full > 0.15, 1, 0)
head(cbind(pred_logi_full, 
           "pred_logi_full_rounded" = round(pred_logi_full, 4), 
           pred_cutoff_15))
```

These are only the first 6 rows in the test set. We can see that the rule works as expected because every estimated loan status below 0.15 is now considered as 0 (no-default), and every estimated loan status above 0.15 is considered as 1 (default). The table above shows how we can create this binary variable given the logistic model prediction.

Note that the rows numbers in the table above are 1, 2, 18, 26, 27 and 28. These are not 1, 2, 3, 4, 5 and 6 because the <tt>`test`</tt> rows were selected randomly out of the <tt>`dat`</tt>. So, the row numbers in the table above correspond to the original place in <tt>`dat`</tt>.

Is <tt>`logi_full`</tt> a good model after all? We can add a new column to the previous table. This new variable represents what really happened with the loan. Then, the first column is the logistic model prediction, the second column the transformed binary variable given a cutoff of 0.15, and the third column is what actually happened (default or no-default). Let's take a sample of 10 observations to conduct the comparison easily. Note that the model correctly predicts a no default in most cases. Rows 308 and 329 predict a default incorrectly and row 323 predict a default correctly.

```{r}
# Let's take from rows 101 to 110.
(cbind(pred_logi_full, pred_cutoff_15, 
           as.data.frame.numeric(test$loan_st)))[101:110,]
```

There is an easy way to evaluate the rest of the cases in the test set using a simple table called confusion matrix.

```{r Confusion matrix at cutoff of 15}
table(Actual = test$loan_st, Predicted = pred_cutoff_15)
```
The `logi_full` model correctly predicts 6,554 no-defaults and 752 defaults. However, it also misclassifies 2,081 defaults as no-defaults and 308 no-defaults as defaults. *How good are these results?* Which of these four values is most important? These are questions we will address later. For now, it is important to note that different models and cutoff rates yield different confusion matrix results. Note that the sum of these four values equals 9,695, which corresponds to the total number of observations in the test set.

You may want to see relative and not absolute values. Let's try the <tt>`CrossTable()`</tt> function instead.

```{r Confusion matrix at cutoff of 15 with CrossTable}
CrossTable(test$loan_st, pred_cutoff_15, 
           prop.r = FALSE, prop.c = FALSE, prop.t = TRUE, 
           prop.chisq = FALSE,
          dnn = c("Actual", "Predicted"))
```

Here, 67.6% of all cases were correctly predicted as non-default (true negatives) while 21.5% were incorrectly predicted as default (false positives), together making up 89.1% of cases and showing non-default is the majority class; meanwhile, 7.8% of all cases were correctly predicted as default (true positives) and 3.2% were incorrectly predicted as non-default (false negatives), comprising 10.9% of cases and indicating default is the minority class, which suggests the model has better prediction accuracy for non-default (67.6% correct) compared to default (7.8% correct), though this should be considered in the context of the imbalanced class distribution.

Instead of arbitrarily consider a cutoff of 0.15, we can follow a different approach. Now consider that we are interested in taking the 20% highest estimates (closer to 1) of <tt>`pred_logi_full`</tt> as default. Equivalently, this is to take the lowest 80% <tt>`pred_logi_full`</tt> estimates (closer to 0) as no-default. Let's calculate the new cutoff that meets this new criterion.

```{r Now 20 highest are default}
# Cutoff definition.
cutoff <- quantile(pred_logi_full, 0.8)
cutoff
```

This new approach (taking the 20% highest estimates of <tt>`pred_logi_full`</tt> as default) represents a cutoff of 0.1994621. Graphically:

```{r}
#| label: fig-fmphncoo0
#| fig-cap: "Full model prediction histogram new cutoff of 0.1994621."
ggplot(pred_logi[pred_logi$model == "pred_logi_full",], 
       aes(x = pred, fill = model)) + 
  geom_density(alpha = 0.4) +
  geom_vline(xintercept = cutoff, linetype = "longdash") +
  labs(y = "Density", x = "Default prediction") +
  theme(legend.position = "none", legend.title = element_blank())
```

Now the cutoff is 0.1994621. This splits the loan status predictions into two parts: higher than the cutoff is a default, and lower than the cutoff is a no-default. Here are the cutoff values as a function of quantiles.

```{r Cutoff by quantiles}
cutoff_all <- quantile(pred_logi_full, seq(0.1, 1, 0.1))
data.frame("cut_off" = round(cutoff_all,5))
```

We can show a similar summary table as we did with the cutoff of 0.15. Here, we show the predictive ability of the <tt>`logi_full`</tt> model with new cutoff of 0.1994621.

```{r Confusion matrix pred_full_20}
# Calculate the predictions with the same model and new cutoff.
pred_full_20 <- ifelse(pred_logi_full > cutoff, 1, 0)
# Show results in a confusion matrix.
CrossTable(test$loan_st, pred_full_20, prop.r = FALSE,
           prop.c = FALSE, prop.t = TRUE, prop.chisq = FALSE)
```

With a cutoff of 0.1994621 we accept 7,309 + 447 = 7,756 applications as those are the ones that the model predicts a no default. Previously, in the case of a cutoff of 0.15 we accept 6,554 + 308 = 6,862. 

Let's compare both confusion matrices:

```{r Cutoff of 15 and 19 comparison}
cat <- c("Correct no-default (true negatives).", 
         "False default (false positives).", 
         "False no-default (false negatives).", 
         "Correct default (true positives).")
cut_15 <- c("67.6%", "21.5", "3.2", "7.8%")
cut_1994621 <- c("75.4%", "13.7%", "4.6%", "6.3%")
cbind("Classification." = cat, cut_15, cut_1994621)
```

Type I and Type II errors are key concepts in hypothesis testing and statistical decision-making, particularly relevant in loan default prediction. A Type I error (false positive) occurs when good customers, who would not default, are incorrectly rejected. On the other hand, a Type II error (false negative) happens when bad customers, who will default, are mistakenly accepted.

The new cutoff of 0.1994621 improves the identification of no-defaults but worsens the identification of defaults. Additionally, the new cutoff results in fewer false positives but more false negatives. This suggests that there is a trade-off between the two outcomes.

We can also look the detail of 0.1994621 cutoff. Comparing two columns, the one in the left with the actual loan status, and the right column with the estimated loan status.

```{r pred_full_20 for a sample of 10 applicants}
# Comparative table in detail.
real_pred_20 <- cbind.data.frame(test$loan_st, pred_full_20,
                     "Did the model succeed?" = test$loan_st == pred_full_20)
# Show some values.
real_pred_20[131:140,]
```

In this sample, the model fails for 1 out of 10 individuals, a fairly good result. Now, let's imagine we are a bank with 9,695 loan applications on our desk (or computer). Suppose we use the predictions from `pred_full_20` to decide whether to accept a loan application. Our model-based acceptance rule is as follows: if `pred_full_20 = 0`, the model predicts no default, and we accept the loan application. According to the excerpt from the table above, we correctly reject applications 399, 417, and 425 because those were indeed defaults. However, we incorrectly reject application 404, which did not default. Overall, using a model as the basis for a decision rule can lead to better outcomes than guessing or applying a random approval policy.

Let's count how many applications are accepted and rejected according to our rule.

```{r Accepted and rejected applications given the model rule}
# Accepted.
accept_20 <- sum(pred_full_20 == 0)
# Rejected.
reject_20 <- sum(pred_full_20 == 1)
data.frame("total" = length(pred_full_20), accept_20, reject_20)
```

Taking the 20% highest estimates of <tt>`pred_logi_full`</tt> as default and the lowest 80% <tt>`pred_logi_full`</tt> as no-default mean that by construction, we accept 7,756 loan applications (80% of the total) and reject 1,939 (20% of the total). Then, the criterion determines the number of accepted applications, and the model determines which applications to accept/reject.

We can evaluate our loan accept/reject rule as we have the corresponding real values in <tt>`loan_st`</tt>. First, let's illustrate the decision making process given the model estimates.

```{r Accept and reject decision}
# First 10 accept decisions.
head(data.frame(real_pred_20[,1:2], 
                decision = ifelse(real_pred_20$pred_full_20 == 0, 
                                  "accept", "reject")), 12)
```

We can add an evaluation column.

```{r Accept and reject decision plus an evaluation}
# First 10 accept decisions.
head(data.frame(real_pred_20[,1:2], 
                decision = ifelse(real_pred_20$pred_full_20 == 0, 
                                  "accept", "reject"),
                evaluation = ifelse(real_pred_20$pred_full_20 == 1, 
                                    "we rejected a good customer", 
 ifelse(real_pred_20$pred_full_20 == 0 & real_pred_20$`test$loan_st` == 0,
        "good decision", "bad decision"))), 12)
```

In the table above, we have the first 12 loan applications. According to our rule, we accept 10 applications and we reject 2 (application #26 and #34). A *good decision* is because we accept the loan that did not default. A *bad decision* is because we accept the loan application and defaulted. We also have some cases in which we rejected a good customer and that is not good. This table above is interesting although a bit problematic as it incorporates a counterfactual approach. In particular, we are evaluating the cases in which we reject the application. A more pragmatic approach is to evaluate our rule according to the cases in which we actually accept the loan application. This is the basically a bad rate measure: *how many accepted loan applications default?*

Remember we accepted 10 and rejected 2 loans. We can identify who default.

```{r A more efficient way to conduct the model evaluation}
# We accept loans that the model predicts a no-default (0).
# In "accepted_loans" we know whether the accepted loans are in fact
# default or no-default.
accepted_loans <- real_pred_20[pred_full_20 == 0, 1]
# The code above says: if we accept the application, tell me what happened.
head(accepted_loans, 10)
```

Note that the third and the tenth application default. These are applications #18 and #37 as expected. Now let's evaluate not 12 applications but all of them, which are 7,756. The bad rate is now expressed as a percentage of the total:

```{r Bad rate calculation}
# bad_rate is the proportion of accepted loans that are in fact default.
bad_rate <- sum(accepted_loans == 1)/length(accepted_loans)
bad_rate
```

This is, by following the model-based rule, we accepted 7,756 loan applications that represent 80% of the total applications. However, 5.76% of those accepted applications were in fact a default. In particular, we accepted 447 loans that are defaults so: $447/7756=0.0576328$.

Models are not perfect but we are always interested to find out a good model that leads to a lower bad rate because (in principle) we do not want to accept many defaults. If we keep the same model, we could reduce this 5.76% by being more strict in the loan application which in simple terms mean to reduce the acceptance rate. This alternative could be controversial as a lower acceptance rate represents lower income (less customers) for the bank or financial firm. In any case, consider we reduce the acceptance rate from 80% to 65% so we can evaluate the impact over the bad rate.

```{r Bad rate when the acceptance rate is 65}
# New cutoff value.
cutoff <- quantile(pred_logi_full, 0.65)
# Split the pred_logi_full into a binary variable.
pred_full_35 <- ifelse(pred_logi_full > cutoff, 1, 0)
# A data frame with real and predicted loan status.
real_pred_35 <- cbind.data.frame(test$loan_st, pred_full_35)
# Loans that we accept given these new rules.
accepted_loans <- real_pred_35[pred_full_35 == 0, 1]
# Bad rate (accepted loan applications that are defaults).
bad_rate <- sum(accepted_loans == 1)/length(accepted_loans)
# Show the bad rate.
bad_rate
```

As expected, the bad rate is lower (from 5.76%% to 3.71%). This is, the lower the acceptance rate the lower the bad rate. In the extreme, if we accept 0 loan applications then our bad rate would be zero, but doing so is equivalent as going out of business. We can create a function such that given a vector of prediction of loan status we can return the bad rate for different cutoff values. This could be useful to build the *bank strategy*. This function will reveal the trade-off between the acceptance rate and the bad rate. In particular, the lower the acceptance rate, the lower the income (bad thing) and the lower the bad rate (good thing). So, *which combination is the optimal?*

## The bank strategy.

A bank could be interested to understand the relationship between the acceptance rate and the bad rate given a model that predicts the loan status.

```{r The bank strategy}
# Function.
bank <- function(prob_of_def){
  cutoff <- rep(NA, 21)
  bad_rate <- rep(NA, 21)
  accept_rate <- seq(1, 0, by = -0.05)
  for (i in 1:21){
    cutoff[i] <- quantile(prob_of_def, accept_rate[i])
    pred_i <- ifelse(prob_of_def > cutoff[i], 1, 0)
    pred_as_good <- test$loan_st[pred_i == 0]
    bad_rate[i] <- sum(pred_as_good == 1)/length(pred_as_good)}
  table <- cbind(accept_rate, cutoff = round(cutoff, 4), 
                 bad_rate = round(bad_rate, 4))
  return(list(table = table, bad_rate = bad_rate, 
              accept_rate = accept_rate, cutoff = cutoff))
  }
```

We can evaluate this function for the <tt>`logi_full`</tt>, and a bad model like the <tt>`logi_age`</tt>. In principle, we expect the <tt>`logi_full`</tt> model to have a more attractive relationship between the acceptance rate and the bad rate. This is, lower bad rates for a given acceptance rate. Any financial institution could be interested in increasing the acceptance rate without increasing too much the bad rate.

Let's apply the function to the <tt>`pred_logi_full`</tt> and the <tt>`logi_age`</tt>.

```{r The bank strategy a good and a bad model}
# Apply the bank function.
bank_logi_full <- bank(pred_logi_full)
bank_logi_age <- bank(pred_logi_age)

data.frame(accept_rate = bank_logi_age$accept_rate,
           "Bad_model_bad_rate" = bank_logi_age$bad_rate, 
           "Good_model_bad_rate)" = bank_logi_full$bad_rate)
```

The full model is superior because at any acceptance rate we can reach a lower bad rate. A plot can reveal the main differences of these two models: <tt>`logi_full`</tt> and <tt>`logi_age`</tt>.

```{r}
#| label: fig-agaabm
#| fig-cap: "A good and a bad model."
# Plot the strategy functions
par(mfrow = c(1, 2))
plot(bank_logi_full$accept_rate, bank_logi_full$bad_rate, 
     type = "l", xlab = "Acceptance rate", ylab = "Bad rate", 
     lwd = 2, main = "logi_full")
abline(v = bank_logi_full[["accept_rate"]][8], lty = 2)
abline(h = bank_logi_full[["bad_rate"]][8], lty = 2)
abline(v = bank_logi_full[["accept_rate"]][5], lty = 2, col = "red")
abline(h = bank_logi_full[["bad_rate"]][5], lty = 2, col = "red")
plot(bank_logi_age$accept_rate, bank_logi_age$bad_rate,
     type = "l", xlab = "Acceptance rate", 
     ylab = "Bad rate", lwd = 2, main = "logi_age")
abline(v = bank_logi_age[["accept_rate"]][8], lty = 2)
abline(h = bank_logi_age[["bad_rate"]][8], lty = 2)
abline(v = bank_logi_age[["accept_rate"]][5], lty = 2, col = "red")
abline(h = bank_logi_age[["bad_rate"]][5], lty = 2, col = "red")
```

The <tt>`logi_full`</tt> model is better because for any acceptance rate we can reach a lower bad rate compared with the <tt>`logi_age`</tt>. This is because the <tt>`logi_full`</tt> model can identify defaults and no defaults with higher precision compared with the <tt>`logi_age`</tt>. The value of a good model is that it can help us to make better business decisions, in this case better credit evaluation decisions.

The model ability to predict defaults and no defaults can be measured by the AUC. The AUC can be defined as the probability that the fit model will score a randomly drawn positive sample higher than a randomly drawn negative sample. AUC stands for area under the curve in the following context:

```{r}
#| label: fig-fmiramib
#| fig-cap: "Full model in red, age model in blue."
ROC_logi_full <- roc(test$loan_st, pred_logi_full)
ROC_logi_age <- roc(test$loan_st, pred_logi_age)
# Draw the ROCs on one plot
plot(ROC_logi_full, col = "red")
lines(ROC_logi_age, col = "blue")
```

Sensitivity is the model ability to correctly identify defaults, these are known as true positive. Specificity is the model ability to correctly identify no-default loans, these are known as true negative.

As expected, the area under the curve (AUC) is higher for the red line which corresponds to the <tt>`logi_full`</tt> model. We can calculate the exact values:

```{r AUC good and bad model}
# Compute the AUCs:
auc(ROC_logi_full)
auc(ROC_logi_age)
```

Note that the <tt>`logi_age`</tt> has an AUC of 0.5301. This is close to a loan approval process in which we randomly accept and reject with no further analysis. In other words, the <tt>`logi_age`</tt> is so bad that it is almost equivalent as using no model at all and accept and reject loan applications based on a random rule. A pure-random approval rule would look like this:

```{r}
#| label: fig-rrm
#| fig-cap: "Random rule model."
set.seed(2020)
pred_rand_model <- runif(length(pred_logi_age))
ROC_rand <- roc(test$loan_st, pred_rand_model)
# Draw the ROCs on one plot
plot(ROC_rand, col = "orange")
auc(ROC_rand)
```

In theory, this random evaluation process would lead to an AUC of $0.5 = (1 \times 1)/2$. In contrast, now imagine we have a perfect model:

```{r}
#| label: fig-pm
#| fig-cap: "Perfect model."
pred_perfect_model <- as.numeric(test$loan_st)
ROC_perfect <- roc(test$loan_st, pred_perfect_model)
plot(ROC_perfect)
auc(ROC_perfect)
```

In a perfect model, the AUC is equal to 1 ($1 \times 1$). The model correctly identify defaults (100% sensitivity) and at the same time the model correctly identify no-defaults (100% specificity).

Edward Malthouse explains these concepts quite well:

```{r}
embed_url("https://youtu.be/HljSmQhLs8M")
```

```{r eval=FALSE, include=FALSE}
log_1_remove_home <- glm(loan_st ~ grade + home + income
+ emp_cat + ir_cat, family = "binomial", data = train)

pred_log_1_remove_home <- predict(log_1_remove_home, newdata = test, 
                                type = "response")
strategy_pred_log_1_remove_home <- 
  bank(as.numeric(pred_log_1_remove_home))
# Plot the strategy functions
plot(bank_logi_full$accept_rate,
     bank_logi_full$bad_rate, 
     type = "l", xlab = "Acceptance rate", ylab = "Bad rate", 
     lwd = 2, main = "logi_full")
abline(v = 0.65, lty = 2)
abline(h = bad_rate, lty = 2)
lines(strategy_pred_log_1_remove_home$accept_rate, 
     strategy_pred_log_1_remove_home$bad_rate,
     type = "l", xlab = "Acceptance rate", 
     ylab = "Bad rate", lwd = 2, main = "logi_multi", col = "red")
abline(v = strategy_pred_log_1_remove_home[["accept_rate"]][8], lty = 2)
abline(h = strategy_pred_log_1_remove_home[["bad_rate"]][8], lty = 2)

```

```{r eval=FALSE, include=FALSE}
#library(glmulti)
#cloglog <- glmulti(loan_st ~ ., family = "binomial", 
 #                     data = train, level =2, 
  #                 plotty = FALSE)

#cloglog <- glm(loan_st~1+l_amnt+income+age+income:l_amnt+age:l_amnt+age:income+grade:age+emp_cat:income+emp_cat:age+ir_cat:l_amnt+ir_cat:income, family = "binomial", 
 #                     data = train)
#predictions_cloglog <- predict(cloglog, newdata = test, type = "response")
#strategy_predictions_cloglog <-
#bank(as.numeric(predictions_cloglog))

# Plot the strategy functions
#par(mfrow = c(1, 2))
#plot(bank_logi_full$accept_rate,
#bank_logi_full$bad_rate,
#type = "l", xlab = "Acceptance rate", ylab = "Bad rate",
##lwd = 2, main = "logi_full")
#abline(v = 0.65, lty = 2)
#abline(h = bad_rate, lty = 2)
#plot(strategy_predictions_cloglog$accept_rate,
#strategy_predictions_cloglog$bad_rate,
##type = "l", xlab = "Acceptance rate",
#ylab = "Bad rate", lwd = 2, main = "logi_age")
#abline(v = strategy_predictions_cloglog[["accept_rate"]][8], lty = 2)
#abline(h = strategy_predictions_cloglog[["bad_rate"]][8], lty = 2)

#plot(bank_logi_full$accept_rate,
####bank_logi_full$bad_rate,
#type = "l", xlab = "Acceptance rate", ylab = "Bad rate",
#lwd = 2, main = "logi_full",
#xlim = c(0.65,1))
#lines(strategy_predictions_cloglog$accept_rate, 
#      strategy_predictions_cloglog$bad_rate, col = "red", lwd = 2)
#abline(v=0.65)
```
