---
title: "**Credit risk**"
author:
- Dr. Mart√≠n Lozano.
- \texttt{<martin.lozano@udem.edu>}
- \texttt{https://sites.google.com/site/mlozanoqf/}
date: "Last compiled on: `r format(Sys.time(), '%d/%m/%Y, %H:%M:%S.')`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    extra_dependencies: ["float"]
    citation_package: natbib
  html_document:
    df_print: paged
fontsize: 12pt
header-includes:
- \usepackage{placeins}
- \usepackage{setspace}
- \usepackage{chngcntr}
- \onehalfspacing
- \counterwithin{figure}{subsection}
- \counterwithin{table}{subsection}
- \usepackage[nottoc]{tocbibind}
toc: yes
bibliography: references.bib
abstract: This document relies on John C. Hull credit risk chapters and Lore Dirick credit risk DataCamp course. Some mathematical background is skipped to emphasize the data analysis, model logic, discussion, graphical approach and R coding (literate programming). This is a work in progress and it is under revision.
---

```{r echo=FALSE}
# This removes all items in environment. 
# It is a good practice to start your code this way.
rm(list=ls())
```

```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
```

```{r eval=FALSE, include=FALSE}
knitr::write_bib(x = c("rmarkdown", "knitr"), file = "references.bib")
```

# Loan analysis.

This section relies on the DataCamp course *Credit Risk Modeling in R* by Lore Dirick. 


## Explore the database.

Let's load the data called \texttt{loan\_data\_ARF.rds} and then understand its structure. This database is similar to the one used by Lore Dirick in the DataCamp course *Credit Risk Modeling in R*. 


```{r Create database, eval=FALSE, include=FALSE}
loan_data <- readRDS("loan_data_ch1.rds")
# Add sex variable.
# Sex = 1 female; sex = 0 male.
set.seed(1)
sex_1 <- rbinom(n = nrow(loan_data[loan_data$loan_status == 1, ]), 
                size = 1, prob = 0.3) # Let's make females default less.
sex_0 <- rbinom(n = nrow(loan_data[loan_data$loan_status == 0, ]), 
                size = 1, prob = 0.55) # Let's make females no default more.
sex <- ifelse(loan_data$loan_status == 1, sex_1, sex_0)
loan_data$sex <- sex
loan_data$sex <- as.factor(loan_data$sex)

# Add region variable.
set.seed(1)
row_def <- nrow(loan_data[loan_data$loan_status == 1, ])
row_nodef <- nrow(loan_data[loan_data$loan_status == 0, ])

def_region_index <- sample(c("N", "E", "W", "S"), size = row_def, 
                           replace  = TRUE, prob = c(0.1, 0.2, 0.3, 0.4))
nodef_region_index <- sample(c("N", "E", "W", "S"), size = row_nodef, 
                           replace  = TRUE, prob = c(0.4, 0.3, 0.2, 0.1))

region <- ifelse(loan_data$loan_status == 1, 
                 def_region_index, nodef_region_index)
loan_data$region <- region
loan_data$region <- as.factor(loan_data$region)

# Imputation.
index_NA <- which(is.na(loan_data$emp_length))
loan_data$emp_length[index_NA] <- median(loan_data$emp_length, na.rm = TRUE)

index_NA <- which(is.na(loan_data$int_rate))
loan_data$int_rate[index_NA] <- median(loan_data$int_rate, na.rm = TRUE)

# New database
saveRDS(loan_data, "loan_data_ARF.rds")
```

```{r Load the modified database}
loan_data <- readRDS("loan_data_ARF.rds")
str(loan_data)
```


This could be a typical database taken from a financial institution like a bank or a firm that uses credit to sell their products or services. Here, we have 29,092 observations of 9 variables. Each observation corresponds to one individual loan and each variable allow us to understand the individual and the loan characteristics. One important variable is \texttt{loan\_status} the value of 0 is no default and the value of 1 is default. A default occurs when a borrower is unable to make timely payments, misses payments, or avoids or stops making payments on interest or principal owed. Then, the definition of default depends on the interests and objectives of the analysis. 

Clearly, this is past information as we know with certainty whether the individual defaulted or not. Past information could be helpful to better understand how likely is that one individual may default according to the rest of their variable values. This kind of data could be easily found in most financial firms as they store details about the applicant and its corresponding loan. Past information will be useful to train our quantitative models and eventually make predictions of new applicants, and even evaluate our predictions.

We can look at the information in different ways. For example, look at the first 10 rows (out of 29,092) and their corresponding 9 variables.

```{r}
# Take a look of the first 10 rows.
head(loan_data, 10)
```

The \texttt{CrossTable} function is used in different contexts. Generating tables like this is only one way we can use it. Here, instead of looking the details of the first 10 rows, we summarize with respect to \texttt{home\_ownership}.

```{r}
library(gmodels)
CrossTable(loan_data$home_ownership)
```

These tables illustrate the data structure and contents. We can also use two variables instead of one. Instead of counting for home ownership we can add a second dimension like \texttt{loan\_status}. This allows us to create more informative tables. 

```{r}
CrossTable(loan_data$home_ownership, loan_data$loan_status, prop.r = TRUE,
prop.c = FALSE, prop.t = FALSE, prop.chisq = FALSE)
```

This table reveals defaults by home ownership. We can use histograms to see one variable distribution. In this case we have the interest rate distribution. 

```{r}
library(ggplot2)
ggplot(loan_data, aes(x = int_rate)) + 
geom_histogram(aes(y=..density..), binwidth = 0.5, colour = "black", 
                fill = "white") # +
#geom_density(alpha = 0.1, fill = "red") 
```


Most of the loans have an interest rate lower than 15%. The following is a similar figure for the annual income.

```{r}
ggplot(loan_data, aes(x = annual_inc)) + 
geom_histogram(aes(y=..density..), colour = "black", fill = "red")
```

The histogram looks suspicious. We have a very large values of the annual income in the horizontal axis and apparently no observations. We can plot the values and explore the data further.

```{r}
ggplot(loan_data, aes(int_rate, annual_inc)) +
geom_point()
```


There are some guys with a very high income. We should explore further and investigate if this are valid observations or simply a mistake in the database.

```{r}
# Extract the row of this high income individuals.
extract <- loan_data[(loan_data$annual_inc > 1000000), ]
extract
```

One guy is not only rich, he is 144 years old. So, my decision is to drop these 9 observations. Remember the database has originally 29,092 rows.

```{r}
index_extract <- data.frame(as.integer(rownames(extract)))
loan_data <- loan_data[-index_extract$as.integer.rownames.extract..,]
ggplot(loan_data, aes(int_rate, annual_inc)) +
geom_point()
```


```{r}
ggplot(loan_data, aes(x = annual_inc)) + 
geom_histogram(aes(y=..density..), colour = "black", fill = "red")
```


## Logistic models.

Logistic models allows us to make predictions about loan defaults. Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable. In this case, the binary dependent variable is default or no default (loan status). 

First, load the data and split it into two sets: (1) training and (2) test. The training set is for building and estimate models and the test set is used to evaluate our model predictions. This is, when estimating models, it is common practice to separate the available data into two portions, training and test data, where the training data is used to estimate parameters and the test data is used to evaluate its accuracy. Because the test data is not used in determining the estimation, it should provide a reliable indication of how well the model is likely to estimate or forecast on new data. In sum, we train the model, we test the model, and once we are OK with the model performance on new data, we are ready to use it in real-life applications.

```{r}
# It is convenient to set the loan_status as factor.
loan_data$loan_status <- as.factor(loan_data$loan_status)
set.seed(567)
index_train <- cbind(runif(1 : nrow(loan_data), 0 , 1), 
                     c(1 : nrow(loan_data)))
index_train <- order(index_train[, 1])
index_train <- index_train[1: (2/3 * nrow(loan_data))]
# Create training set: training_set
training_set <- loan_data[index_train, ]
# Create test set: test_set
test_set <- loan_data[-index_train, ]
```

We have 29,083 observations in \texttt{loan\_data}. The code above randomly selects $29083 \times (2/3)=19388$ rows to form the \texttt{training\_set}. The \texttt{test\_set} are the remaining $29083-19388=9695$ rows. The random selection is highly recommended as the \texttt{loan\_data} may have some structure or sorting that could bias our model estimation and negatively impact our model test. 

Take a look of the training set structure.

```{r}
# See the data structure.
str(training_set)
```

Variables as factors are useful for model estimation and data visualization. Factors are variables in R which take on a limited number of different values; such variables are often refered to as categorical variables. 

Assume we think that the \texttt{loan\_status} depends on the age of the individual. We can estimate a simple logistic model to learn about the relationship between age and loan status.

```{r}
# Fitting a simple logistic model.
log_model_age <- glm(loan_status ~ age , family = "binomial", 
                     data = training_set)
log_model_age
```


Apparently, there is a negative relationship between age and loan status. The AIC value (13580) is useful when comparing models. The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data. 

Let's estimate another simple model where the interest rate category is used as a predictor of the \texttt{loan\_status}.

```{r}
# Build a glm model with variable int_rate as a predictor.
log_model_ir <- glm(formula = loan_status ~ int_rate, family = "binomial", 
                     data = training_set)
# Print the parameter estimates.
log_model_ir
```

The AIC is a lower (13220), so we have a better model now. 

Using one single predictor as age or interest rate could be a limited approach. Let's add some more. Also, let's introduce the \texttt{summary} function to extract more information about the model estimation. The \texttt{log\_model\_multi} below assumes that the loan status depend on the age, interest rate, grade, loan amount, and annual income. 

```{r}
# Multiple variables in a logistic regression model.
log_model_multi <- glm(loan_status ~ age + int_rate + grade + log(loan_amnt) + 
                         log(annual_inc) , family = "binomial", 
                       data = training_set)
# Obtain significance levels using summary().
summary(log_model_multi)
```

Our multi-factor model works well. In \texttt{log\_model\_multi}, the AIC value is the lowest so far (13050), so this should be considered as the best model at the moment. The \texttt{summary} function shows the significance levels of the estimators, but we are currently more interested in the goodness of fit of the models because we are interested in conducting predictions about the \texttt{loan\_status}. This is, we are interested to use a model to find out whether new applicants are expected to default or not, rather than in the credit risk factors. 

When someone fill out a credit application form, we collect information but we do not know for sure whether she or he will eventually default. A credit risk model can help us in this task.

## Prediction and model evaluation.

Let's take our three models: \texttt{log\_model\_age}, \texttt{log\_model\_ir} and \texttt{log\_model\_multi} from the previous subsection to carry out a simple prediction exercise. We start this prediction exercise by identifying one observation in the test set and ask the models to estimate the \texttt{loan\_status}. Every model is expected to produce different estimates.

```{r}
# Define one single observation in test_set.
test_case <- as.data.frame(test_set[1,])
test_case
```

We know in advance that the \texttt{loan\_status} of this observation taken from the test set is 0. In other words, a good credit risk model should estimate a no default.

The values of \texttt{loan\_status} in the database is either 0 or 1. However, the logistic models estimate the \texttt{loan\_status} as values in the range of 0 to 1. This mean that we would expect the estimated \texttt{loan\_status} to be close to 0. 

```{r}
# Estimate the loan status with log_model_age and log_model_ir models.
log_model_age_pred <- as.numeric(predict(log_model_age, newdata = test_case, 
                              type = "response"))
# Remember the test_case is only one observation (one row).
log_model_ir_pred <- as.numeric(predict(log_model_ir, newdata = test_case, 
                              type = "response"))
log_model_multi_pred <- as.numeric(predict(log_model_multi, newdata = test_case, 
                              type = "response"))
predictions <- rbind("log_model_age"   = log_model_age_pred, 
                     "log_model_ir"    = log_model_ir_pred, 
                     "log_model_multi" = log_model_multi_pred)
colnames(predictions) <- "predictions of a known no-default"
predictions
```

These values are low as they are close to 0. We could interpret this as a certain ability of the models to predict this single case from the test set. However, several questions remains unanswered and requires further analysis. For example: How can we determine if the prediction is low enough to consider it as a non-default? We may need a cut-off to decide. We will explore this later.

Another aspect is: What about the rest of the cases in the test set? We have 9,695 observations in the test set and in the example above we only test for the first one. We are interested in test for the entire test set, not only for one observation. Fortunately, this issue is easy to address as we only need to change the \texttt{newdata} parameter in the \texttt{predict} function. In particular, instead of \texttt{newdata = test\_case} (which is one observation) we can change it to \texttt{newdata = test\_set} (which is the entire 9,695 test set).

```{r}
# Estimate the loan status with the three models.
pred_log_model_age <- predict(log_model_age, newdata = test_set, 
                              type = "response")
# Now newdata = test_set so we are testing all the test set observations.
pred_log_model_ir <- predict(log_model_ir, newdata = test_set, 
                             type = "response")
pred_log_model_multi <- predict(log_model_multi, newdata = test_set, 
                                type = "response")
# The range of the estimated loan status.
log_model_age_predall <- range(pred_log_model_age)
log_model_ir_predall <- range(pred_log_model_ir)
log_model_multi_predall <- range(pred_log_model_multi)

predictions_2 <- rbind("log_model_age"   = log_model_age_predall, 
                       "log_model_ir"    = log_model_ir_predall,
                       "log_model_multi" = log_model_multi_predall)
aic <- rbind(log_model_age$aic, log_model_ir$aic, log_model_multi$aic)
predictions_2 <- cbind(predictions_2, aic)
colnames(predictions_2) <- c("lower value", "higer value", "AIC")
predictions_2
```

Narrow ranges could be problematic because the model could not be able to differentiate between defaults (predictions closer to 1) and no-defaults (predictions closer to 0). The higher AIC corresponds to the worst model and the lower AIC to the best model.

Let's explore the \texttt{log\_model_age}:

```{r}
library(tidyr)
pred_log_model_age <- data.frame(pred_log_model_age)
pred_log_model_age <- gather(pred_log_model_age)

ggplot(pred_log_model_age, aes(x = value, fill = key)) + 
  geom_density(alpha = 0.4)
```
The \texttt{log\_model_age} fails to predict values ranging from 0 to 1. In fact, these values are quite concentrated in a very small range of values. As a consequence, this model fails to differentiate between default and no default predictions. A boxplot reveals the same failure.

This does not look very good as the maximum values for loan status are concentrated around 0.12. Let's visualize the predictions of the \texttt{log\_model\_ir} and \texttt{log\_model\_multi}.

```{r}
pred_age_ir_multi <- as.data.frame(cbind(pred_log_model_ir,
                                   pred_log_model_multi))
pred_age_ir_multi <- gather(pred_age_ir_multi)
ggplot(pred_age_ir_multi, aes(x = value, fill = key)) + 
  geom_density(alpha = 0.4)
```

Presumably, a model which considers all available predictors could be better for predicting the \texttt{loan\_status}. Remember we estimate models using the training set data and then we use the test set to conduct the predictions. Evaluating a model performance with the same data as we use to estimate the model is problematic. 

```{r}
# Logistic regression model using all available predictors in the data set.
log_model_full <- glm(loan_status ~ age + int_rate + grade + log(loan_amnt) + 
                        log(annual_inc) + emp_length + home_ownership + sex +
                      region, family = "binomial", data = training_set)
# Loan status predictions for all test set elements.
predictions_all_full <- predict(log_model_full, newdata = test_set, 
                                type = "response")
# Look at the predictions range.
range(predictions_all_full)
```

Now, the range is wider. A wider range means that the estimates are now closer to 1. This is good because we need the model to be able to predict both no-defaults (0) and defaults (1). Let's see the corresponding plot.


```{r}
pred_log_model_age <- data.frame(pred_log_model_age)
pred_log_model_age <- gather(pred_log_model_age)

ggplot(pred_log_model_age, aes(x = value, fill = key)) + 
  geom_density(alpha = 0.4)
```

```{r}
predictions_ir_full <-as.data.frame(cbind(pred_log_model_ir,
                                   predictions_all_full))
predictions_ir_full <- gather(predictions_ir_full)

ggplot(predictions_ir_full, aes(x = value, fill = key)) + 
  geom_density(alpha = 0.4)
```

The \texttt{log\_model\_full} model predictions looks better than the other models.

Another question is: How can we know these model predictions corresponds to a default or no default? The loan status predictions go from 0.004236739 to 0.601424125. At the end, these loan status estimations need to be interpreted as a default or no default because we are interested on that. Are they closer enough to 0 to consider a no default? This issue is addressed by setting up a cutoff rate so we can split all estimated loan status into 0 or 1.

Let's arbitrarily consider a cutoff of 0.15. This mean that every estimated loan status below 0.15 will be considered as 0 (no-default), and every estimated loan status above 0.15 will be considered as 1 (default). Graphically looks like this:

```{r}
library(dplyr)
pred_log_model_full <- data.frame(predictions_all_full)
pred_log_model_full <- gather(pred_log_model_full)
pred_log_model_full <- mutate(pred_log_model_full, 
                              def = ifelse(value < 0.15, 0, 1))
pred_log_model_full$def <- as.factor(pred_log_model_full$def)

ggplot(pred_log_model_full, aes(x = value, fill = def)) + 
  geom_density(alpha = 0.4, adjust = 0.4)
```


Here, red represent no default and blue default. Let's set up the rule to convert the estimated loan status into a binary variable 0 or 1. 

```{r}
# Specifying a cut-off.
# Make a binary predictions-vector using a cut-off of 15%
pred_cutoff_15 <- ifelse(predictions_all_full > 0.15, 1, 0)
```

See how this transformation takes place:

```{r}
head(cbind(predictions_all_full, pred_cutoff_15))
```
These are only the first 6 rows not all of them. We can see that the rule works well as every estimated loan status below 0.15 is now considered as 0 (no-default), and every estimated loan status above 0.15 is considered as 1 (default). The table above show how we can create this binary variable given the logistic model prediction. 

Note that the rows numbers in the table above are 1, 2, 18, 26, 27 and 28. These are not 1, 2, 3, 4, 5 and 6 because the \texttt{test\_set} rows were selected randomly out of the \texttt{loan\_data}. So, the row numbers in the table above correspond to the original place in \texttt{loan\_data}.

Is \texttt{log\_model\_full} a good model after all? We can add a new column to the previous table. This new variable represents what really happened with the loan. Then, the first column is the logistic model prediction, the second column the transformed binary variable given a cutoff of 0.15 and the third column is what actually happened (default or no-default).

```{r}
# Let's take from rows 101 to 110.
(cbind(predictions_all_full, pred_cutoff_15, 
           as.data.frame.numeric(test_set$loan_status)))[101:110,]
```
Note that the model correctly predict a no default in most cases. Row 318 predict a default incorrectly and row 323 predict a default correctly. There is an easy way to evaluate the rest of the cases in a simple table called confusion matrix. 

```{r}
# Construct a confusion matrix.
table(test_set$loan_status, pred_cutoff_15)
```

The model predicts 6,780 of no-defaults correctly and 430 defaults correctly. However, the model predicts 1,899 defaults that are in fact no-defaults and 588 no-defaults that are in fact defaults. How good are these results? This is a question we address later. For now, we can say that different models and different cutoff rates lead to different confusion matrix results.

You may want to see relative and not absolute values. So we could divide by the number of observations in the test set.

```{r}
table(test_set$loan_status, pred_cutoff_15)/length(test_set$loan_status)
```
However, this is not as informative. Let's try a different approach and use the \texttt{CrossTable} function instead.

```{r}
CrossTable(test_set$loan_status, pred_cutoff_15, prop.r = TRUE,
prop.c = FALSE, prop.t = FALSE, prop.chisq = FALSE)
```
This table is more informative. The model correctly predicts no-defaults in 78.1% of all the observed no-default cases. In other words, the model fails to predict no-default in 21.9% of the total no-default cases. Now the default. The model correctly predicts the default in 42.2% of all the observed default cases. Alternatively, the model fails to predict default in 57.8% of the total default cases.

Instead of arbitrarily consider a cutoff of 0.15, we can follow a different approach. Now consider that we are interested in taking the 20% highest estimates (closer to 1) as default. Equivalently, this is to take the lowest 80% estimates (closer to 0) as no-default.


```{r}
# Cutoff definition.
cutoff <- quantile(predictions_all_full, 0.8, na.rm = TRUE)
cutoff
```
Take the lowest 80% estimates (closer to 0) as no-default represent a cutoff of 0.1600801. Graphically:

```{r}
# Graphically.
h <- hist(predictions_all_full, breaks = 100, plot = FALSE)
ccat = cut(h$breaks, c(-Inf, cutoff, Inf))
plot(h, col = c("green", "red")[ccat])
```

Now the cutoff is 0.1600801. This splits the loan status predictions into two parts: higher than the cutoff is a default (red), and lower than the cutoff is a no-default (green). Taking the lowest 80% estimates (closer to 0) as no-default is an arbitrary decision. Here are the cutoff values depending on this arbitrary decision.


```{r}
cutoff_all <- quantile(predictions_all_full, seq(0.1, 1, 0.1), na.rm = TRUE)
data.frame(cutoff_all)
```

We can show a similar summary table as we did with the cutoff of 0.15. Here, we show the predictive ability of the \texttt{log\_model\_full} model and new cutoff of 0.1600801.

```{r}
# Calculate the predictions with the same model and new cutoff.
pred_full_20 <- ifelse(predictions_all_full > cutoff, 1, 0)
# Show results in a confusion matrix.
CrossTable(test_set$loan_status, pred_full_20, prop.r = TRUE,
           prop.c = FALSE, prop.t = FALSE, prop.chisq = FALSE)
```
With a cutoff of 0.1600801 we accept 7,757 applications as those are the ones that the model predicts a no default. We can compare both confusion matrix:

```{r}
cat <- c("correct no-default", "false default", 
         "false no-default", "correct default")
cut_15 <- c(0.781, 0.219, 0.578, 0.422)
cut_16 <- c(0.819, 0.181, 0.640, 0.360)
cbind(cat, cut_15, cut_16)
```
 
The new cutoff of 0.1600801 improves the identification of no-defaults but worsen the identification of default. Also, the new cutoff of 0.1600801 fails less in the default and fails more in the no-default.

We can also look the detail of 0.1600801 cutoff. Comparing two columns, the one in the left with the actual loan status, and the right column with the estimated loan status.

```{r}
# Comparative table in detail.
true_and_predval <- cbind.data.frame(test_set$loan_status, pred_full_20)
# Show some values.
true_and_predval[101:110,]
```
These sample of 10 results are the same as in the cutoff of 0.15. We fail to correctly identify a no default at 318 and correctly identify a default in 323.

Let's imagine we are a bank. We have a total of 9,697 applications for a loan. Assume we use the predictions from \texttt{pred\_full\_20} to decide whether we accept a loan application or not. Our model-based acceptance rule is the following: if \texttt{pred\_full\_20 = 0} then the model estimates a no-default and we accept the loan application. According to the extract of table above, we fortunately reject application 323 because that was indeed a default. However, we reject application 318 incorrectly because it did not default. 

Let's count how many applications are accepted and rejected according to our rule.

```{r}
# Accepted.
accept_20 <- sum(pred_full_20 == 0)
# Rejected.
reject_20 <- sum(pred_full_20 == 1)
data.frame(accept_20, reject_20)
```
By following this rule, we accept 7,757 loan applications (80% of the total) and reject 1,940 (20% of the total). We can evaluate our loan application rule as we have the corresponding real values in \texttt{loan\_status}.

```{r}
# We accept loans that the model predicts a no-default (0).
# In "accepted_loans" we know whether the accepted loans are in fact
# default or no-default.
accepted_loans <- true_and_predval[pred_full_20 == 0, 1]
# The code above says: if we accept the application, tell me what happened.
head(accepted_loans, 10)
```
Here we have the first 10 accepted loans. We fail at the second (row 2) and the ninth (row 37) as the \texttt{true\_and\_predval} indicates:

```{r}
head(true_and_predval, 13)
```


```{r}
# bad_rate is the proportion of accepted loans that are in fact default.
bad_rate <- sum(accepted_loans == 1)/length(accepted_loans)
bad_rate
```

This is, by following the model-based rule, we accepted 7,757 loan applications that represent 80% of the total applications. However, 8.4% of those accepted applications were in fact a default. Models are not perfect but we are always interested to find out a good model that leads to a lower bad rate. If we keep the same model and the test set the same, we could reduce this 8.4% by being more strict in the loan application which in simple terms mean to reduce the acceptance rate. This alternative could be controversial as a lower acceptance rate could represent lower income for the bank or financial firm. In any case, consider we reduce the acceptance rate from 80% to 65% so we can evaluate the impact over the bad rate.

```{r}
# New cutoff value.
cutoff <- quantile(predictions_all_full, 0.65)
# Split the predictions_all_full into a binary variable.
pred_full_35 <- ifelse(predictions_all_full > cutoff, 1, 0)
# A data frame with real and predicted loan status.
true_and_predval <- cbind.data.frame(test_set$loan_status, pred_full_35)
# Loans that we accept given these new rules.
accepted_loans <- true_and_predval[pred_full_35 == 0, 1]
# Bad rate (accepted loan applications that are defaults).
bad_rate <- sum(accepted_loans == 1)/length(accepted_loans)
# Show the bad rate.
bad_rate
```

As expected, the bad rate is lower (from 8.4% to 7.1%). This is, the lower the acceptance rate the lower the bad rate. We can create a function such that given a vector of prediction of loan status we can return the bad rate for different cutoff values. This could be useful to build the bank strategy more easily. This function will reveal the trade-off between the acceptance rate and the bad rate. In particular, the lower the acceptance rate, the lower the income (bad thing) and the lower the bad rate (good thing). So, which combination is the optimal?

## The bank strategy.

A bank could be interested to understand the relationship between the acceptance rate and the bad rate given a model that predicts the loan status.

```{r}
# Function.
strategy_bank <- function(prob_of_def){
cutoff <- rep(NA, 21)
bad_rate <- rep(NA, 21)
accept_rate <- seq(1, 0, by = -0.05)
for (i in 1:21){
  cutoff[i] <- quantile(prob_of_def, accept_rate[i])
  pred_i <- ifelse(prob_of_def > cutoff[i], 1, 0)
  pred_as_good <- test_set$loan_status[pred_i == 0]
  bad_rate[i] <- sum(pred_as_good == 1)/length(pred_as_good)}
table <- cbind(accept_rate, cutoff = round(cutoff, 4), 
               bad_rate = round(bad_rate, 4))
return(list(table = table, bad_rate = bad_rate, 
            accept_rate = accept_rate, cutoff = cutoff))
}
```

We can evaluate this function for the \texttt{log\_model\_full} and the bad model called \texttt{log\_model}. In principle, we expect the \texttt{log\_model\_full} model to have a more attractive relationship between the acceptance rate and the bad rate. This is, lower bad rates for a given acceptance rate. Any financial institution could be interested in increasing the acceptance rate without increasing too much the bad rate.

Let's apply the function to the \texttt{predictions\_all\_full} and the \texttt{log\_model}.

```{r}
# Apply the strategy_bank function.
strategy_predictions_all_full <- 
  strategy_bank(as.numeric(predictions_all_full))
strategy_pred_log_model <- strategy_bank(as.numeric(pred_log_model_age))
data.frame(accept_rate = strategy_pred_log_model$accept_rate,
           log_model_bad_rate = strategy_pred_log_model$bad_rate, 
           full_model_bad_rate = strategy_predictions_all_full$bad_rate)
```

The full model is superior because at any acceptance rate we can reach a lower bad rate. A plot can reveal the main differences of these two models: \texttt{log\_model\_full} and \texttt{log\_model}.

```{r}
# Plot the strategy functions
par(mfrow = c(1, 2))
plot(strategy_predictions_all_full$accept_rate,
     strategy_predictions_all_full$bad_rate, 
     type = "l", xlab = "Acceptance rate", ylab = "Bad rate", 
     lwd = 2, main = "log_model_full")
abline(v = strategy_predictions_all_full[["accept_rate"]][8], lty = 2)
abline(h = strategy_predictions_all_full[["bad_rate"]][8], lty = 2)
abline(v = strategy_predictions_all_full[["accept_rate"]][5], lty = 2, 
       col = "red")
abline(h = strategy_predictions_all_full[["bad_rate"]][5], lty = 2, 
       col = "red")
plot(strategy_pred_log_model$accept_rate, 
     strategy_pred_log_model$bad_rate,
     type = "l", xlab = "Acceptance rate", 
     ylab = "Bad rate", lwd = 2, main = "log_model_age")
abline(v = strategy_pred_log_model[["accept_rate"]][8], lty = 2)
abline(h = strategy_pred_log_model[["bad_rate"]][8], lty = 2)
abline(v = strategy_pred_log_model[["accept_rate"]][5], lty = 2, 
       col = "red")
abline(h = strategy_pred_log_model[["bad_rate"]][5], lty = 2, col = "red")
```

The \texttt{log\_model\_full} model is better because for any acceptance rate we can reach a lower bad rate compared with the \texttt{log\_model}. This is because the \texttt{log\_model\_full} model can identify defaults and no defaults with higher precision compared with the \texttt{log\_model}. The value of a good model is that it can help us to make better business decisions, in this case better credit evaluation decisions.

The model ability to predict defaults and no defaults can be measured by the AUC. The AUC can be defined as the probability that the fit model will score a randomly drawn positive sample higher than a randomly drawn negative sample. AUC stands for area under the curve in the following context:

```{r}
# Load the pROC-package
library("pROC")
ROC_all_full <- roc(test_set$loan_status, predictions_all_full)
ROC_log_model <- roc(test_set$loan_status, pred_log_model_age)
# Draw the ROCs on one plot
plot(ROC_all_full, col = "red")
lines(ROC_log_model, col = "blue")
```

Sensitivity is the model ability to correctly identify defaults, these are known as true positive. Specificity is the model ability to correctly identify no-default loans, these are known as true negative.

As expected, the area under the curve (AUC) is higher for the red line which corresponds to the \texttt{log\_model\_full} model. We can calculate the exact values:

```{r}
# Compute the AUCs
auc(ROC_all_full)
auc(ROC_log_model)
```

Note that the \texttt{log\_model} has an AUC of 0.5125. This is close to a loan approval process in which we randomly accept and reject with no further analysis. In other words, the \texttt{log\_model} is so bad that it is equivalent as using no model at all and accept and reject loan applications based on a random rule. A pure-random approval rule would look like this:

```{r}
set.seed(2020)
pred_rand_model <- runif(length(pred_log_model_age))
ROC_rand <- roc(test_set$loan_status, pred_rand_model)
# Draw the ROCs on one plot
plot(ROC_rand, col = "orange")
auc(ROC_rand)
```

In theory, this random evaluation process would lead to an AUC of $0.5 = (1 \times 1)/2$. In contrast, now imagine we have a perfect model:

```{r}
pred_perfect_model <- as.numeric(test_set$loan_status)
ROC_perfect <- roc(test_set$loan_status, pred_perfect_model)
plot(ROC_perfect)
auc(ROC_perfect)
```

In a perfect model, the AUC is equal to 1 ($1 \times 1$). The model correctly identify defaults (100% sensitivity) and at the same time the model correctly identify no-defaults (100% specificity)

```{r eval=FALSE, include=FALSE}

log_1_remove_home <- glm(loan_status ~ grade + home_ownership + annual_inc
+ emp_cat + ir_cat, family = "binomial", data = training_set)

pred_log_1_remove_home <- predict(log_1_remove_home, newdata = test_set, 
                                type = "response")
strategy_pred_log_1_remove_home <- 
  strategy_bank(as.numeric(pred_log_1_remove_home))
# Plot the strategy functions
plot(strategy_predictions_all_full$accept_rate,
     strategy_predictions_all_full$bad_rate, 
     type = "l", xlab = "Acceptance rate", ylab = "Bad rate", 
     lwd = 2, main = "log_model_full")
abline(v = 0.65, lty = 2)
abline(h = bad_rate, lty = 2)
lines(strategy_pred_log_1_remove_home$accept_rate, 
     strategy_pred_log_1_remove_home$bad_rate,
     type = "l", xlab = "Acceptance rate", 
     ylab = "Bad rate", lwd = 2, main = "log_model_multi", col = "red")
abline(v = strategy_pred_log_1_remove_home[["accept_rate"]][8], lty = 2)
abline(h = strategy_pred_log_1_remove_home[["bad_rate"]][8], lty = 2)

```

```{r eval=FALSE, include=FALSE}
#library(glmulti)
#cloglog <- glmulti(loan_status ~ ., family = "binomial", 
 #                     data = training_set, level =2, 
  #                 plotty = FALSE)

#cloglog <- glm(loan_status~1+loan_amnt+annual_inc+age+annual_inc:loan_amnt+age:loan_amnt+age:annual_inc+grade:age+emp_cat:annual_inc+emp_cat:age+ir_cat:loan_amnt+ir_cat:annual_inc, family = "binomial", 
 #                     data = training_set)
#predictions_cloglog <- predict(cloglog, newdata = test_set, type = "response")
#strategy_predictions_cloglog <-
#strategy_bank(as.numeric(predictions_cloglog))

# Plot the strategy functions
#par(mfrow = c(1, 2))
#plot(strategy_predictions_all_full$accept_rate,
#strategy_predictions_all_full$bad_rate,
#type = "l", xlab = "Acceptance rate", ylab = "Bad rate",
##lwd = 2, main = "log_model_full")
#abline(v = 0.65, lty = 2)
#abline(h = bad_rate, lty = 2)
#plot(strategy_predictions_cloglog$accept_rate,
#strategy_predictions_cloglog$bad_rate,
##type = "l", xlab = "Acceptance rate",
#ylab = "Bad rate", lwd = 2, main = "log_model_age")
#abline(v = strategy_predictions_cloglog[["accept_rate"]][8], lty = 2)
#abline(h = strategy_predictions_cloglog[["bad_rate"]][8], lty = 2)

#plot(strategy_predictions_all_full$accept_rate,
####strategy_predictions_all_full$bad_rate,
#type = "l", xlab = "Acceptance rate", ylab = "Bad rate",
#lwd = 2, main = "log_model_full",
#xlim = c(0.65,1))
#lines(strategy_predictions_cloglog$accept_rate, 
#      strategy_predictions_cloglog$bad_rate, col = "red", lwd = 2)
#abline(v=0.65)
```

# The Merton model.

The Merton model is useful when we are interested to evaluate the credit risk of public firms. In short, it evaluates how likely is that the value of the firm's assets fall in the future below a certain threshold represented by the firm's debt. By doing that, the model is able to estimate a probability of default among other results. The model can be used as a tool to propose changes (for example) in the balance sheet to reduce the credit risk exposure of a firm.

## Minimize a function in R.

The estimation of the credit risk Merton's model requires a minimization of a function. In this section, we show a simple minimization example. Here are some useful online resources for further references.

* [FRM: How d2 in Black-Scholes becomes PD in Merton model.](https://youtu.be/lV0ytJYbVzc)
* [Normal Probability Distribution Graph. Interactive.](https://www.intmath.com/counting-probability/normal-distribution-graph-interactive.php)
* [How to use optim in R.](https://www.r-bloggers.com/how-to-use-optim-in-r/)

The objective function to be minimized is $f(a,b)=\sum(a + bx_i-y_i)^2$. We have six values for $x$ and $y$, and we are expected to minimize the function $f$ by finding the parameter values $a$ and $b$. The $x_i$ and $y_i$ data looks like this:

```{r}
library(knitr) # To show formatted tables (kable).
# Data.
dat <- data.frame(x = c(1, 2, 3, 4, 5, 6), y = c(1, 3, 5, 6, 8, 12))
kable(dat, caption = "Observations.", row.names = TRUE) # The table.
```

Graphically:

```{r fig.cap="Original data."}
# A scatter plot.
plot(y ~ x, data = dat, pch = 19, cex = 2)
```

Let's implement the minimization.

```{r}
# Function that calculates the residual sum of squares.
min.RSS <- function(data, par) {
  with(data, sum((par[1] + par[2] * x - y)^2)) }
# Function minimization.
result <- optim(par = c(0, 1), min.RSS, data = dat)
a <- result$par[1]
b <- result$par[2]
f <- sum((a + b * dat$x - dat$y)^2)
# Minimization output.
ans <- data.frame(a, b, f)
kable(ans, caption = "Minimization results f(a,b).")
```

The minimization result is represented by a line, and the line is characterized by an intercept equal to $-1.266846$ and a slope equal to $2.02862$. 

We can plot it and verify that this is indeed the right answer.

```{r fig.cap="Line that minimize the residual sum of squares."}
# View the results.
plot(y ~ x, data = dat, pch = 19, ylim = c(-2, 12), xlim = c(0, 6), cex = 2)
abline(a, b, col = "red", lwd = 3)
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
```

The function that we minimize here is basically the OLS criterion. This is why the regression model leads to the same results.

```{r}
# The previous example is equivalent as a linear regression model.
reg <- lm(y ~ x, data = dat)
# See the results.
summary(reg)
```

The function that we minimize in the estimation of the credit risk Merton model is more elaborated but we follow the same principle as above.

## Applied example.

Let's follow the example 24.3 in [@Hull]. Five parameters: $E_0=3$, $\sigma_E=0.8$, $rf=0.05$, $T=1$, $D=10$ lead to an estimate of the value of the assets today $V_0$ and the volatility of the assets $\sigma_V$. With these seven parameters we can estimate the probability of default (pd) among other results.

```{r}
# List of known parameters.
E0 <- 3 # Value of the equity as today, I recommend market capitalization.
se <- 0.8 # Stock returns volatility.
rf <- 0.05 # Risk-free rate.
TT <- 1 # Maturity.
D <- 10 # Value of the debt. The Bloomberg function DDIS is useful.
# We need equations 24.3 and 24.4 to estimate V0 and sv.
eq24.3 <- function(V0, sv) { 
  ((V0 * pnorm((log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) -
  D * exp(-rf * TT) * pnorm(((log(V0 / D) +
  (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) - sv * sqrt(TT)) - E0)) }
eq24.4 <- function(V0, sv) { 
  ((pnorm((log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) *
      sv * V0 - se * E0)) }
# Footnote 10 indicates that we should minimize F(x,y)^2+G(x,y)^2
min.footnote10 <- function(x) 
  (eq24.3(x[1], x[2]))^2 + (eq24.4(x[1], x[2]))^2
# The minimization leads to the values of V0 and sv.
V0_sv <- optim(c(1, 1), min.footnote10)
# Define the values as parameters.
V0 <- V0_sv$par[1]
sv <- V0_sv$par[2]
# Calculate the probability of default as a function.
PD <- function(V0, sv) { 
  pnorm(-(((log(V0 / D) + (rf + sv^2 / 2) * TT) / 
             (sv * sqrt(TT))) - sv * sqrt(TT))) }
# Calculate the probability of default given the previous parameters.
pd <- PD(V0, sv)
# Gather the results.
ans <- data.frame(pd, V0, sv)
kable(ans, caption = "Main model results.")
```

Let's analyze the role of the values of $d_1$, $d_2$, $N(d_1)$ and $N(d_2)$. First we isolate these values.

```{r}
# Inspect the role of d and N(d).
d1 <- (log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))
d2 <- d1 - sv * sqrt(TT)
Nd1 <- pnorm(d1)
Nd2 <- pnorm(d2)
ans <- data.frame(d1, Nd1, d2, Nd2)
kable(ans, caption = "Inspect the role of d and N(d).")
```

Now let's illustrate how $N(-d_2)$ lead to the probability of default given the properties of a standard normal distribution.

```{r fig.cap="The red area represents the probability of default."}
xseq <- seq(-4, 4, 0.01) 
densities <- dnorm(xseq, 0, 1) # Standard normal distribution.
# Graphical evaluation of N(-d_2).
plot(xseq, densities, xlab = "d values (d_2 is dotted line)", 
     ylab = "Density", type = "l", lwd = 2, cex = 2)
abline(h = 0)
polygon(c(xseq[xseq >= d2], d2), c(densities[xseq >= d2], 0), col = "red")
polygon(c(xseq[xseq < d2], d2), c(densities[xseq < d2], 0), col = "blue")
legend("topleft", legend = c("N(d_2)=0.8730604 
is represented in blue and
N(-d_2)=0.1269396 in red.

N(d_2)+N(-d_2)=1"),
bg = "white", bty = "n", cex = 0.9)
abline(v = d2, lty = 2, lwd = 2)
```

Now let's analyze the minimization that leads to $V_0$ and $\sigma_V$. We do not have an analytic math expressions (closed form) to calculate $V_0$ and $\sigma_V$, instead we have approximate values or estimates given the minimization of equations 24.3 and 24.4. In order to see how good this approximation is, we can retrieve the value of $[F(x,y)]^2+[G(x,y)]^2$ evaluated at $x=V_0$ and $y=\sigma_v$. This value is supposed to be positive as both $F(x,y)$ and $G(x,y)$ functions are squared, so we expect a positive number close to zero when evaluated at $x=V_0$ and $y=\sigma_v$.

```{r}
# Evaluate how good is the minimization.
min.footnote10(x = c(V0, sv))
```

The minimization conducted in the function \texttt{min.footnote10} above worked fairly well.

There is another way to approach how the minimization work. We can propose a graphical analysis. In particular, we can verify whether $V_0=12.39578$ and $\sigma_V=0.2123041$ minimizes the objective function. To illustrate this in two axis, we need to fix either $V_0$ or $\sigma_V$, and evaluate $[F(x,y)]^2+[G(x,y)]^2$ with different values of $V_0$ or $\sigma_V$. Let's do that.

```{r}
# Fix sv and evaluate different values of V0.
V0.seq <- seq(from = 4, to = 20, length.out=50)
sv.rep <- rep(sv, 50)
# Function to be evaluated.
FG <- function(V0, sv) { (eq24.3(V0, sv))^2 + (eq24.4(V0, sv))^2 }
# Apply the function with fixed sv and different V0 values.
FG.V0 <- mapply(FG, V0.seq, sv.rep)
```


```{r fig.cap="Here we fix the volatility of the assets and change the value of the assets at time zero. The minimization looks fine."}
# Plot the results.
plot(V0.seq, FG.V0, type = "l", xlab = expression(paste(V[0])),
     ylab = expression(paste(F(x,y)^2+G(x,y)^2)), cex.lab = 0.8, lwd = 3)
abline(v = V0, lty = 2)
abline(h = FG(V0, sv), lty = 2)
points(V0, FG(V0, sv), pch = 1, cex = 3, col = "red", lwd = 2)
```

Clearly the minimization worked well here. For the sake of completeness, now we fix the $V_0$ value and evaluate the function again.

```{r fig.cap="Here we fix the value of the assets at time zero and change the volatility of the assets. The minimization looks fine."}
# Now fix V0, and evaluate different sv values.
sv.seq <- seq(0, 0.6, length.out = 50)
V0.rep <- rep(V0, 50)
# Evaluate the function with these parameters.
FG.sv <- mapply(FG, V0.rep, sv.seq)
# Plot the results.
plot(sv.seq, FG.sv, type = "l", xlab = expression(paste(sigma[V])), 
     ylab = expression(paste(F(x, y)^2 + G(x, y)^2)), 
     cex.lab = 0.8, lwd = 3)
abline(v = sv, lty = 2)
abline(h = FG(V0, sv), lty = 2)
points(sv, FG(V0, sv), pch = 1, cex = 3, col = "red", lwd = 2)
```

We can show the same as before but in a 3D plot.

```{r fig.cap="Minimization of the function."}
# outer evaluates the function for each combination of the vectors.
z <- outer(V0.seq, sv.seq, FG) # z is now 50x50 matrix.
persp(V0.seq, sv.seq, z, col = "springgreen", shade = 0, xlab = "V0",
      ylab = "sv", zlab = "Probability of default")
```
This green plot is nice but it is not interactive. We could plot interactive three-dimension plots with other R packages like \texttt{plotly} and show them in a html context or java environment. Given that this is a PDF document, we cannot show dynamic plots (at least not yet), so we propose to use contour plots. Contour plots or level plots are a way to show a three-dimensional surface on a two-dimensional plane.

```{r fig.cap="Minimization of the function: a contour view."}
# The nlevels is high to emphasize the minimum value in blue.
contour(V0.seq, sv.seq, z, xlab = "V0", ylab = "sv", lwd = 2, nlevels = 300)
points(V0, sv, pch = 19, col = "blue", cex = 2)
```

The following plot cannot be seen in a PDF output, but it can be seen in a browser as an interactive plot.

```{r eval=FALSE}
library(plotly)
p <- plot_ly(x = V0.seq, y = sv.seq, z = z, type = "surface") %>%
layout(
  title = "Minimization of f at V0=12.395 an sv=0.212",
  scene = list(xaxis = list(title = "V0"), yaxis = list(title = "sv"),
    zaxis = list(title = "f=F^2+G^2"))) 
add_markers(p, x = 12.3957765, y = 0.2123041, z = 0, inherit = TRUE)
```

The Merton model allows us to calculate more values in this credit risk assessment. 

```{r}
# Other results in the Merton's model.
market_value_debt <- V0 - E0
pv_promised_payment_debt <- D * exp(-rf * TT)
Expected_Loss <- (pv_promised_payment_debt - market_value_debt) /
  pv_promised_payment_debt
recovery_rate <- 1 - (Expected_Loss / pd)
ans <- data.frame(market_value_debt, pv_promised_payment_debt, 
                  Expected_Loss, recovery_rate, pd)
kable(t(ans), caption = "Other results in the Merton's model.")
```

Could we figure out the implied bond yield spread? It would be similar to the fair interest rate that this firm has to pay given its probability of default and recovery rate. Take Hull's equation 24.2 as a reference: $\lambda(T)=S(T)/(1-R)$ and solve for $S(T)$: $S(T)=\lambda(T)\times(1-R)$, so $S(T)=(0.12693963)\times(1-0.90350393)$. This is 122.4918 basis points more than the risk-free rate. Then, with five parameters we can calculate (among other things) the firm's probability of default and the correspondent yield spread of this firm's bond. It would be interesting to compare this value with the market yield spread and evaluate whether the market yield is over or underestimated according to our theoretical value. 

```{r}
spread <- function(E0, se, rf, TT, D) { 
  eq24.3 <- function(V0, sv) { 
  ((V0 * pnorm((log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) -
  D * exp(-rf * TT) * pnorm(((log(V0 / D) +
  (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) - sv * sqrt(TT)) - E0)) }
eq24.4 <- function(V0, sv) { 
  ((pnorm((log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) *
      sv * V0 - se * E0)) }
  min.footnote10 <- function(x) 
  (eq24.3(x[1], x[2]))^2 + (eq24.4(x[1], x[2]))^2
V0_sv <- optim(c(1, 1), min.footnote10)
V0 <- V0_sv$par[1]
sv <- V0_sv$par[2]
pd <- pnorm(-(((log(V0 / D) + (rf + sv^2 / 2) * TT) /
          (sv * sqrt(TT))) - sv * sqrt(TT)))

market_value_debt <- V0 - E0
pv_promised_payment_debt <- D * exp(-rf * TT)
Expected_Loss <- (pv_promised_payment_debt - market_value_debt) /
  pv_promised_payment_debt
recovery_rate <- 1 - (Expected_Loss / pd)
spread <- pd * (1-recovery_rate)
spread
}
```

```{r}
spread(E0 = 3, se = 0.8, rf = 0.05, TT = 1, D = 10)
```


```{r}
x.T <- seq(0.5, 10, 0.5)
spread.T <- mapply(spread, E0 = 3, se = 0.8, rf = 0.05, x.T, D = 10)
```


```{r}
data.frame(x.T, spread.T)
```

## Inside view of the Merton's model.

The model assumes that the assets follow a geometric Brownian stochastic process. Let's assume the daily evolution of the market value of the firm assets over a year. Here are 10 simulated paths of $V_t$ from $t=t,...,T$. The reference of this section is Chapter 14 and 15 (John C. Hull).

```{r fig.cap="Simulation of 10 paths of the assets."}
library(Sim.DiffProc)
# Merton assumes V follows a geometric brownian motion process.
V0.sim <- function(i) {  # the argument i is not used here.
  GBM(N = 365, T = 1, t0 = 0, x0 = V0, theta = 0.05, sigma = sv)
  }
set.seed(3) # Reproducibility  
paths <- sapply(1:10, V0.sim) # Create 10 paths for V.
# Plot results.
matplot(paths, type ="l", col = "black", lwd = 1, lty = 1,
        ylab = expression(paste(V[t])), xlab = "Time in days (0 to T)")
abline(h = D, col = "red", lwd = 2)
points(1, V0, pch = 19, cex = 1.5, col = "blue")
```

These simulations can be interpreted as the evolution of the value of the firm's assets in 10 parallel universes. In the past, some students have called this figure *la gr√°fica de los pelitos*, this is OK as long as you understand the figure implications. The right name is 10 simulated geometric brownian processes. Let's count the cases in which $V_t<D$ note that here I use $t$ and not $T$, so we are counting the cases in which at least at some time $t$ the value of the assets were lower than 10. 

```{r}
# Which path went lower than D?
which(colSums(paths < 10) > 0)
```
The value of the assets was lower than $10 at some time in the second, fourth and ninth alternate universes. Let's plot these cases.

```{r fig.cap="Note that the blue path finally did not default. Recovery is possible as in the real life."}
# There might be an easier way to select these cases.
matplot(paths[,which(colSums(paths < 10) > 0)], type = "l", 
        col = c("green", "blue", "red"), lwd = 2, lty = 1,
        ylab = expression(paste(V[t])), xlab = "Time in days (0 to T)")
abline(h = D, col = "red", lwd = 2)
points(1, V0, pch = 19, cex = 1.5, col = "blue")
```

The blue path was lower than 10 at some time, but at $t=T$ it is clearly higher than 10. For this reason, the blue path does not represent a firm's default. See how the assets are not high enough to pay the debt at maturity only in two cases: green and red. Thus, according to this simulation the probability of default is 20%. Here is how we can calculate the cases in which $V_T<D$.

```{r}
sum(paths[366,] < 10) / 10
```

The probability of default of 20% can be disappointing because it is significantly higher than 0.12693963. This is because the number of simulations is small. The following examples show how these values tend to converge as we increase the number of simulations from 100 to 100,000.

Here are 100 simulated paths of $V_0$ to $V_T$.

```{r fig.cap="Simulation of 100 paths of the assets."}
set.seed(3) # Reproducibility  
paths <- sapply(1:100, V0.sim) # Create 100 paths.
# Plot results.
matplot(paths, type = "l", col = "black", lwd = 1, lty = 1,
        ylab = expression(paste(V[t])), xlab = "Time in days (0 to T)")
abline(h = D, col = "red", lwd = 2)
points(1, V0, pch = 19, cex = 1.5, col = "blue")
```

Given the simulation above, the probability of default is 17%. This is closer to 12.693963%.

```{r}
sum(paths[366,] < 10) / 100
```

Let's see all the 17 cases that end in default.

```{r fig.cap="Default cases."}
# These are the 17 default cases.
matplot(paths[,which(paths[366,] < 10)], type = "l", col = "black", 
        lwd = 1, lty = 1, ylab = expression(paste(V[t])), 
        xlab = "Time in days (0 to T)")
abline(h = D, col = "red", lwd = 2)
points(1, V0, pch = 19, cex = 1.5, col = "blue")
```

Now, let's see the 11 cases in which $V_t$ eventually went lower than $10 but at the end did not default. 

```{r fig.cap="All these paths went lower than 10 at some point, but at the end the value of the assets are higher than 10. These are paralell universes in which the firm finally survived after some drama."}
# V_t < 10
almost_default <- which(colSums(paths < 10) > 0)
# V_T < 10
default <- which((paths[366,] < 10))
# There should be an easier way to do this:
matplot(paths[,almost_default[which(almost_default %in% default == FALSE)]], 
        type = "l", col = c(1:11), lwd = 1, lty = 1, 
        ylab = expression(paste(V[t])), xlab = "Time in days (0 to T)")
abline(h = D, col = "red", lwd = 2)
points(1, V0, pch = 19, cex = 1.5, col = "blue")
```

What if the asset value is decreasing over time $V_t \leq V_0$? Just as in the case of a physical assets that depreciates over time. What if we are interested in evaluating $V_t<D$ at any time, not only at maturity? Ideally, we should consider an alternative to the geometric brownian motion stochastic process in order to conduct our simulation analysis. However, we could consider a shortcut. This is, take the same GBM stochastic process and filter those paths which are $V_t \leq V_0$. There are some minor pitfalls here. For example, we still have some paths which show a decrease and then an increase. Those cases can be interpreted as if the depreciation is lower than an asset value increase. In any case, our approach delivers paths which never lead to an asset value higher than $V_0$. 

```{r}
set.seed(3) # Reproducibility 
paths <- sapply(1:100, V0.sim) # Create 100 paths.
# Select those paths with all 366 values lower or equal than V0.
select.paths <- which(colSums(paths <= V0) == 366)
new.paths <- paths[,select.paths]
# Plot.
matplot(new.paths,
        type = "l", col = c(1:100), lwd = 1, lty = 1,
        ylab = expression(paste(V[t])), 
        xlab = "Time in days (0 to T)")
abline(h = D, col = "red", lwd = 2)
points(1, V0, pch = 19, cex = 1.5, col = "blue")
# Probability
sum(new.paths < 10) / length(new.paths)
```

Here, only path number 49 out of 100 meet our condition. This path meets $V_t<D$ in 47.54098% of the cases. Let's increase the number of simulations because 100 was apparently rather small. With 1,000 simulations the probability is 43.23445%. Let's see the case of 10,000 simulations.

```{r}
set.seed(3) # Reproducibility 
paths <- sapply(1:10000, V0.sim) # Create 10,000 paths.
# Select those paths with all 366 values lower or equal than V0.
select.paths <- which(colSums(paths <= V0) == 366)
new.paths <- paths[,select.paths]
# Plot.
matplot(new.paths,
        type = "l", col = c(1:10000), lwd = 1, lty = 1,
        ylab = expression(paste(V[t])), 
        xlab = "Time in days (0 to T)")
abline(h = D, col = "red", lwd = 2)
points(1, V0, pch = 19, cex = 1.5, col = "blue")
# Probability
sum(new.paths < 10) / length(new.paths)
```

Now the probability is 35.96873%. We can consider this one as the final result for our purposes.

Here are 100,000 simulated $V_T$ showed in a histogram. The reference for this section is 15.1 (John C. Hull). Note that instead of drawing the complete paths, we can directly get the distribution of $V_T$. The resulting distribution is log-normal.

```{r fig.cap="Histogram of 100,000 values of the assets at maturity."}
set.seed(13)
# 100,000 values of VT at once.
# Here, I added a TT value twice. This was not a mistake, because in the tutorial this value was equal to one. However, my students are trying with different maturities. For a formal reference please see equation 15.3 in Hull.
VT <- exp(rnorm(100000, log(V0) + (0.05 - (sv^2) / 2)*TT, sv*sqrt(TT)))
# Plot results.
# You may need to change the 100 if colors do not show properly. Sometimes this value is too high and bins are so many that it looks all black.
h <- hist(VT, 100, plot = FALSE)
ccat <- cut(h$breaks, c(-Inf, D, Inf))
# You may also need to change the xlim if a big red square appears as changes in maturity may imply larger values of VT. I recommend trying without xlim and see.
plot(h, main = NULL, col = c("red", "blue")[ccat], 
     xlab = "Value of the assets at maturity", xlim = c(4, 30))
legend("topright", 
legend = c("The red area is 12.657%, and represents
the simulated probability of default.
The probability of default of the model
following the textbook example is 
N(-d_2)=12.69396%. As we can see,
both are very close."),
col = c("red", "blue"), pch = c(19), bg = "white", bty = "n", cex = 0.8)
```

The probability of default can be calculated as:

```{r}
sum(VT < 10) / 100000
```

Note that 0.12657 is now much closer than 0.12693963. Convergence achieved.

This firm can be represented as an European call option payoff. The payoff of a typical stock option is $c=max(S_T-K, 0)$ or equivalently in terms of Merton's model: $E_T=max(V_T-D, 0)$.

```{r fig.cap="A view of the firm's balance sheet in the future. Looks like a typical European call option payoff."}
ET <- pmax(VT - D, 0) # payoff function of a typical call option.
plot(sort(VT), sort(ET), type = "l", lwd = 4, xlim = c(5, 20), 
     ylim = c(0, 10), xlab = "Simulated assets at maturity (V_T)",
     ylab ="Simulated equity at maturity (E_T)")
abline(v = 10, lty = 2)
legend("right", legend = c("E_T=max(V_T-D,0). Given the
simulation, E_T=0 happens in 
12.657% of the cases.
According to the model, E_T=0
happens in 12.69396% of the 
cases."),
bg = "white", bty = "n", cex = 0.7)
```

This diagram show the relevant balance sheet accounts in the future, at maturity. This is because the $E_T$ is a function of $V_T$ and $D$. As long as $V_T<D$, then the value of $E_T=0$. Otherwise, the value of $E_T= V_T-D$, just as the accounting equation suggest but in future terms. Note that the Merton's model say nothing about the estimate value of $E_T$, instead we have an estimate about how likely is that $V_T<D$, or in other words how likely is that $E_T<0$.

Then, we know that $E_T= max(V_T-D, 0)$. The Black‚ÄìScholes‚ÄìMerton formula can also estimate the theoretical value of $E_0$. This is, if the observable market value given by the market capitalization available in any financial site is $E_0=3$, we can retrieve the theoretical value of $E_0$ so we can compare whether the firm observable market value is over or undervalued. In other words, the Black-Scholes-Merton model can give us an estimate of the firm value. In particular, the model gives the value of the equity today as:

$E_0=V_0N(d_1)-De^{-rT}N(d_2)$

This is equation 24.3 in Hull. To estimate $E_0$, we need $V_0$ and $\sigma_V$ and these two values were already estimated before with the implementation of the min.footnote10 function above. It is true that this minimization requires $E_0$, the point here is to use the observable market capitalization $E_0=3$ to find out the unobsevable $V_0$ and $\sigma_V$ and then estimate the theoretical value of $E_0$.

The code is:

```{r}
# Black-Scholes call.
E0.theo <- function(V0, D, rf, TT, sv) {
  d1 <- (log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))
  d2 <- d1 - sv * sqrt(TT)
  c <- V0 * pnorm(d1) - D * exp(-rf * TT) * pnorm(d2)
}
V0 <- 12.39578 # Assets.
D <- 10 # Debt.
rf <- 0.05 # Risk-free rate.
TT <- 1 # Maturity.
sv <- 0.2123041 # Volatility of assets.
(E0.theo(V0, D, rf, TT, sv))
```
Then, we can argue that the market value of the firm is slightly undervalued since the theoretical value of the firm is 3.000357 compared with 3. This approach opens the possibility to value firms. It also opens the possibility to compare the book value in the Balance sheet of the total assets versus the estimate $V_0$. This will allow us to understand the difference between book value and market value of the assets, not only of the equity. We can even implement a similar analysis about the market value of the debt just as we did before. 

## The probability of default as a function of some parameters.

We can expand our analysis by evaluating how changes in parameters change the probability of default in the context of the Merton model. This is interesting because we can move from estimating a probability of default to propose changes in the firm's parameters like the capital structure to reduce a firm's probability of default. First, we need the probability of default as a function.

```{r}
pd <- function(E0, se, rf, TT, D) { 
  eq24.3 <- function(V0, sv) { 
  ((V0 * pnorm((log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) -
  D * exp(-rf * TT) * pnorm(((log(V0 / D) +
  (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) - sv * sqrt(TT)) - E0)) }
eq24.4 <- function(V0, sv) { 
  ((pnorm((log(V0 / D) + (rf + sv^2 / 2) * TT) / (sv * sqrt(TT))) *
      sv * V0 - se * E0)) }
  min.footnote10 <- function(x) 
  (eq24.3(x[1], x[2]))^2 + (eq24.4(x[1], x[2]))^2
V0_sv <- optim(c(1, 1), min.footnote10)
V0 <- V0_sv$par[1]
sv <- V0_sv$par[2]
pd <- pnorm(-(((log(V0 / D) + (rf + sv^2 / 2) * TT) /
          (sv * sqrt(TT))) - sv * sqrt(TT)))
pd }
```

Now we have a convenient function: $pd = f(E_0, \sigma_E, rf, T, D)$. Remember these five parameters are required to estimate $V_0$ and $\sigma_V$ before calculating $pd$.

```{r}
# Evaluate the case of the textbook example.
pd1 <- pd(3, 0.8, 0.05, 1, 10)
pd1
```


Now, we create vectors of parameters and evaluate the \texttt{pd} function.

```{r}
l = 50 # Vectors of length 50.
# Create vectors of the parameters.
x.E <- seq(from = 1, to = 20, length.out = l)
x.rf <- seq(0, 4, length.out = l)
x.D <- seq(1, 20, length.out = l)
x.T <- seq(0.5, 20, length.out = l)
x.se <- seq(0, 3, length.out = l)
# Evaluate the pd at different values.
pd.E <- mapply(pd, x.E, se, rf, TT, D)
pd.rf <- mapply(pd, E0, se, x.rf, TT, D)
pd.D <- mapply(pd, E0, se, rf, TT, x.D)
pd.T <- mapply(pd, E0, se, rf, x.T, D)
pd.se <- mapply(pd, E0, x.se, rf, TT, D)
```


```{r}
library(bazar) # para la funci√≥n almost.equal
pd.E # vemos qu√© hay en pd.E
# Busco el valor de pd m√°s parecido a 0.05
index <- which(almost.equal(pd.E, 0.05, tolerance = 0.01))
# ¬øQu√© n√∫mero de observaci√≥n es? (da 38 con los datos del tutorial)
index
# ¬øQu√© valor de equity me da una pd de 0.05?
x.E[index] # da 15.34694 con los datos del tutorial.
pd.E[index] # da 0.04952178 con los datos del tutorial.
```

Let's plot the probability of default as a function of equity. What we are doing here is to evaluate the probability of default function 50 times, as the $E_0$ has 50 values from 1 to 20. By doing this, we end up with 50 values of the probability of default. 

```{r fig.cap="The probability of default as a function of the equity at time zero."}
plot(x.E, pd.E, type = "l", ylab = "Probability of default", 
     xlab = "Equity at time zero", lwd = 3, ylim = c(0, 0.15))
lines(x.E, pd.E)
abline(v = E0, lty = 2)
abline(h = pd1, lty = 2)
points(E0, pd1, pch = 1, cex = 3, col = "red", lwd = 2)
```

These relationships between the parameters and the probability of default are all non-linear. This is interesting because according to the model, the current levels of the parameters are important when deciding which parameter might lead to a high or low impact over the probability of default.

```{r fig.cap="The probability of default as a function of the risk-free rate."}
plot(x.rf, pd.rf, type = "l", ylab = "Probability of default", 
     xlab = "Risk-free rate", lwd = 3)
abline(h = 0, lty = 2)
abline(v = rf, lty = 2)
abline(h = pd1, lty = 2)
points(rf, pd1, pch = 1, cex = 3, col = "red", lwd = 2)
```

Probability of default as a function of debt. Note that adding 1 or subtracting 1 unit of debt has different impact over the probability of default.

```{r fig.cap="The probability of default as a function of the debt."}
plot(x.D, pd.D, type = "l", ylab = "Probability of default", 
     xlab = "Debt", lwd = 3, ylim = c(0, 0.15))
abline(h = 0, lty = 2)
abline(v = D, lty = 2)
abline(h = pd1, lty = 2)
points(D, pd1, pch = 1, cex = 3, col = "red", lwd = 2)
```

Probability of default as a function of maturity. Note that asking for a higher debt maturity (as in a negotiation), do not lead to a lower probability of default if we keep the rest of the parameters unchanged in our firm. Imagine we arrange a meeting with the bank manager and we ask for more time to pay our debt. The bank manager should not (in principle) extend the debt maturity as long as we commit to do some changes in the firm. For example, we can negotiate a longer maturity and promise to increase the firm's equity.

```{r fig.cap="The probability of default as a function of the debt's maturity."}
plot(x.T, pd.T, type = "l", ylab = "Probability of default", 
     xlab = "Maturity", lwd = 3)
abline(h = 0, lty = 2)
abline(v = TT, lty = 2)
abline(h = pd1, lty = 2)
points(TT, pd1, pch = 1, cex = 3, col = "red", lwd = 2)
```

Probability of default as a function of the volatility of the equity $\sigma_E$.

```{r fig.cap="The probability of default as a function of the volatility of the equity."}
plot(x.se, pd.se, type = "l", ylab = "Probability of default",
     xlab = "Standard deviation of equity", lwd = 3)
abline(h = 0, lty = 2)
abline(v = se, lty = 2)
abline(h = pd1, lty = 2)
points(se, pd1, pch = 1, cex = 3, col = "red", lwd = 2)
```


In the previous plots we evaluate the probability of default function by changing one of the following parameters: $E_0$, $\sigma_E$, $rf$, $T$ or $D$. We were able to do that because we constructed a vector of 50 different values for these five parameters. Note that $V_0$ and $\sigma_V$ do not remain fixed because they are at the same time a function of $E_0$, $\sigma_E$, $rf$, $T$ and $D$. Now, let's start by plotting the probability of default as a function of $E_0$ and $\sigma_E$.

One alternative is to evaluate the probability of default function 50 times by taking the 50 pairs of values of $E_0$ and $\sigma_E$. By doing that, we will end with three vectors of size 50 that we can plot as a three dimension scatter plot.

```{r fig.cap="The probability of default as a function of the equity at time zero and the volatility of equity."}
library(scatterplot3d)
ED <- mapply(pd, x.E, x.se, rf, TT, D)
p.ED <- scatterplot3d(x.E, x.se, ED, pch = 16, type = "h", color = 1:50, 
                      angle = 100, xlab = "Equity at time zero", 
                      ylab = "Volatility of equity", 
                      zlab = "Probability of default")
p.ED$points3d(E0, se, pd1, type = "h", col = "red", pch = 20, cex = 3)  
```

The red point represent the case of the original textbook example in which $pd = f(E_0=3, \sigma_E=0.8,rf=0.05,T=1,D=10)=0.1269396$. Note that the red point is not part of the 50 plotted observations simply because there is no case in which the probability of default function is evaluated in $E_0=3, \sigma_E=0.8$. Although the plot above is not incorrect, it might be incomplete as we are not showing all possible values of the probability of default. One alternative to show a more complete plot is to evaluate all possible combinations of $E_0$ and $\sigma_E$ to evaluate the probability of default function. If we do that, we will have 50 values for $E_0$ and $\sigma_E$ that represent the $x$ and $y$ axis, and a $50 \times 50$ matrix containing the probability of default. This allows us to plot a surface plot and a contour plot.

```{r}
# Create the empty matrix.
p_E_se <- matrix(0, nrow = l, ncol = l)
# Fill the empty matrix with probability of default values.
for(i in 1 : l){ # Is there an easier way to do this?
  for(j in 1 : l){
    p_E_se[i,j] <- mapply(pd, x.E[i], x.se[j], rf, TT, D) } }
```

Let's plot the results.

```{r fig.cap="The probability of default as a function of the equity at time zero and the volatility of equity: A plane or surface view."}
# Plot results.
p.ED1 <- persp(x.E, x.se, p_E_se, zlab = "pd", xlab = "Equity at time zero", 
               ylab = "Volatility of equity", theta = 60, phi = 10, 
               expand =  0.5, col = "orange", shade = 0.2, 
               ticktype = "detailed") 
# Add the original pd value as in the textbook example.
points(trans3d(E0, se, pd1, p.ED1), cex = 2, pch = 19, col = "blue")
```

Now the plot becomes a plane. A contour plot simplify the reading of a three dimension plot by reducing it into two dimensions: $x$ and $y$ coordinates. The value of the probability of default is represented by the contour lines. 

```{r fig.cap="The probability of default as a function of the equity at time zero and the volatility of equity: A contour view."}
contour(x.E, x.se, p_E_se, xlab = "Equity at time zero", 
        ylab = "Volatility of equity", lwd = 2)
points(E0, se, pch = 19, col = "blue", cex = 2)
```      

The blue point is the original case. Note that the value is slightly higher than 0.1, which is consistent with the $pd=0.1269396$.

## GoT: capital structure.

The problem. Daenerys Targaryen owns a big manufacturing firm that produces fire extinguishers. Surprisingly, the firm data corresponds exactly as the data in the example 24.3. She is planning to ask the Iron Bank of Braavos for a loan using the peaceful civilized way (without dragons). However, she would like to reduce the probability of default of the firm first, in that way she might negotiate better credit conditions. Daenerys has some understanding about very basic finance because she knows that she could either reduce the debt, or increase the capital in order to reduce the probability of default. Doing both alternatives at the same time is clearly more expensive so she is not very keen about it. 

What is the best strategy to reduce the probability of default? Reduce the debt by 2 or increase the capital by 2?

```{r}
D.seq <- seq(from = 0, to = 13, by = 0.1)
# Evaluate pd function: E0 changes from 1 to 5; debt goes from 0 to 13.
E1 <- mapply(pd, 1, se, rf, TT, D.seq)
E2 <- mapply(pd, 2, se, rf, TT, D.seq)
E3 <- mapply(pd, 3, se, rf, TT, D.seq)
E4 <- mapply(pd, 4, se, rf, TT, D.seq)
E5 <- mapply(pd, 5, se, rf, TT, D.seq)
```

Plot the results.

```{r fig.cap="GoT problem. Probability of default and capital structure."}
# We use these vectors for colors and legends in the following plots.
colors <- c("green", "purple", "black", "blue", "red")
leg <- c("E0=1", "E0=2", "E0=3 (initial value)", "E0=4", "E0=5")
# Plot.
plot(D.seq, E1, type = "l", col = "green", lwd = 3,
     xlab = "Debt value. Initially, D=10",
     ylab = "Probability of default. Initially PD=12.69%")
lines(D.seq, E2, col = "purple", lwd = 3)
lines(D.seq, E3, col = "black", lwd = 3)
lines(D.seq, E4, col = "blue", lwd = 3)
lines(D.seq, E5, col = "red", lwd = 3)
abline(v = D, lty = 2)
abline(h = pd1, lty = 2)
legend("bottomright", legend = leg, lwd = rep(3, 5), col = colors, 
       bg = "white")
points(D, pd1, pch = 19, cex = 2)
```

Let's illustrate the alternatives as clear as possible.

```{r fig.cap="GoT solution. Probability of default and capital structure."}
plot(D.seq, E1, type = "l", col = "green", lwd = 3,
     xlab = "Debt value. Initially, D=10",
     ylab = "Probability of default. Initially PD=12.69%")
lines(D.seq, E2, col = "purple", lwd = 3)
lines(D.seq, E3, col = "black", lwd = 3)
lines(D.seq, E4, col = "blue", lwd = 3)
lines(D.seq, E5, col = "red", lwd = 3)
abline(v = D, lty = 2)
abline(v = 8, lty = 2)
abline(h = pd1, lty = 2)
abline(h = pd(E0, se, rf, TT, 8), lty = 2)
abline(h = pd(5, se, rf, TT, D), lty = 2)
legend("bottomright", legend = leg, lwd = rep(3, 5), col = colors, 
       bg = "white")
points(D, pd1, pch = 19, cex = 2)
points(D, pd(5, se, rf, TT, D), pch = 19, cex = 2, col = "red")
points(8, pd(3, se, rf, TT, 8), pch = 19, cex = 2, col = "yellow")
```

Consider a different initial situation. Now, $E_0=2$ and $D=2$ and the rest remains the same. In this case, the probability of default is now 7.13%. What is the best strategy to reduce the probability of default? Reduce the debt by 1 or increase the capital by 1?

```{r fig.cap="GoT problem 2. Probability of default and capital structure."}
leg2 <- c("E0=1", "E0=2 (initial value)", "E0=3", "E0=4", "E0=5")

plot(D.seq, E1, type = "l", col = "green", lwd = 3,
     xlab = "Debt value. Initially, D=2",
     ylab = "Probability of default. Initially PD=7.13%", 
     xlim = c(0, 2.5), ylim = c(0, 0.08))
lines(D.seq, E2, col = "purple", lwd = 3)
lines(D.seq, E3, col = "black", lwd = 3)
lines(D.seq, E4, col = "blue", lwd = 3)
lines(D.seq, E5, col = "red", lwd = 3)
abline(v = 2, lty = 2)
abline(h = pd(2, se, rf, TT, 2), lty = 2)
legend("bottomright", legend = leg2, lwd = rep(3, 5), col = colors, 
       bg = "white", cex = 0.6)
points(2, pd(2, se, rf, TT, 2), pch = 19, cex = 2, col = "purple")
```

The solution can be illustrated as follows:

```{r fig.cap="GoT solution 2. Probability of default and capital structure."}
plot(D.seq, E1, type = "l", col = "green", lwd = 3,
     xlab = "Debt value. Initially, D=2",
     ylab = "Probability of default. Initially PD=7.13%", 
     xlim = c(0, 2.5), ylim = c(0, 0.08))
lines(D.seq, E2, col = "purple", lwd = 3)
lines(D.seq, E3, col = "black", lwd = 3)
lines(D.seq, E4, col = "blue", lwd = 3)
lines(D.seq, E5, col = "red", lwd = 3)
abline(v = 2, lty = 2)
abline(v = 1, lty = 2)
abline(h = pd(2, se, rf, TT, 2), lty = 2)
abline(h = pd(2, se, rf, TT, 1), lty = 2)
abline(h = pd(E0, se, rf, TT, 2), lty = 2)
legend("bottomright", legend = leg2, lwd = rep(3, 5), col = colors, 
       bg = "white", cex = 0.6)
points(2, pd(2, se, rf, TT, 2), pch = 19, cex = 2, col = "purple")
points(1, pd(2, se, rf, TT, 1), pch = 19, cex = 2, col = "yellow")
points(2, pd(E0, se, rf, TT, 2), pch = 19, cex = 2)
```

Interesting.

Consider a third problem. In a parallel universe, Daenerys Targaryen's firm has some liquidity short-term troubles. She needs either more cash now or some more time to pay her current debt. She would like to know which alternative leads to the lowest increase of the probability of default. Add $2 more to her current debt, or ask for a half year more time to pay her current debt? 

```{r}
T1 <- mapply(pd, E0, se, rf, 0.5, D.seq)
T2 <- mapply(pd, E0, se, rf, 1, D.seq)
T3 <- mapply(pd, E0, se, rf, 1.5, D.seq)
T4 <- mapply(pd, E0, se, rf, 2, D.seq)
T5 <- mapply(pd, E0, se, rf, 2.5, D.seq)
```

```{r fig.cap="GoT problem 3. More debt or longer maturity?"}
leg3 <- c("T=2.5", "T=2", "T=1.5", "T=1", "T=0.5")

plot(D.seq, T5, type = "l", col = "green", lwd = 3,
     xlab = "Debt value. Initially, D=6",
     ylab = "Probability of default. Initially PD=20.33%")
lines(D.seq, T4, col = "purple", lwd = 3)
lines(D.seq, T3, col = "black", lwd = 3)
lines(D.seq, T2, col = "blue", lwd = 3)
lines(D.seq, T1, col = "red", lwd = 3)
abline(v = 6, lty = 2)
abline(h = pd(E0, se, rf, 1.5, 6), lty = 2)
legend("bottomright", legend = leg3, lwd = rep(3, 5), col = colors, 
       bg = "white")
points(6, pd(E0, se, rf, 1.5, 6), pch = 19, cex = 2)
```

Let's visualize the result.

```{r fig.cap="GoT answer 3. More debt or longer maturity?"}
plot(D.seq, T5, type = "l", col = "green", lwd = 3,
     xlab = "Debt value. Initially, D=6",
     ylab = "Probability of default. Initially PD=20.33%")
lines(D.seq, T4, col = "purple", lwd = 3)
lines(D.seq, T3, col = "black", lwd = 3)
lines(D.seq, T2, col = "blue", lwd = 3)
lines(D.seq, T1, col = "red", lwd = 3)
abline(v = 6, lty = 2)
abline(v = 8, lty = 2)
abline(h = pd(E0, se, rf, 1.5, 6), lty = 2)
abline(h = pd(E0, se, rf, 2, 6), lty = 2)
abline(h = pd(E0, se, rf, 1.5, 6), lty = 2)
abline(h = pd(E0, se, rf, 1.5, 8), lty = 2)
legend("bottomright", legend = leg3, lwd = rep(3, 5), col = colors, 
       bg = "white")
points(6, pd(E0, se, rf, 1.5, 6), pch = 19, cex = 2)
points(8, pd(E0, se, rf, 1.5, 8), pch = 19, cex = 2, col = "yellow")
points(6, pd(E0, se, rf, 2, 6), pch = 19, cex = 2, col = "purple")
```

You are expected to discuss the results and implications.

\newpage
# The Gaussian copula model.

```{r echo=FALSE}
# This removes all items in environment. 
# It is a good practice to start your code this way.
rm(list=ls())
```

Copulas allow us to decompose a joint probability distribution into their marginals (which by definition have no correlation) and a function which couples (hence the name) them together and thus allows us to specify the correlation separately. The copula is that coupling function. Here, we introduce the simplest type of copulas into a very common problem in credit risk which is the time to default.  

## The basics.

In a finance-context, the variable $x$ represents a firm's performance measure that goes from $-4$ to $4$ in the horizontal axis. Strictly speaking, more extreme values of $x$ like $-\infty$ and $+\infty$ are theoretically possible but are very rare and happen quite infrequently at least in real-life situations. At the moment, we do not care too much about what kind of performance measure this is, it could be liquidity for example, solvency, or any other that it is normalized to have values between $-4$ and $4$. In a statistics-context, we can think that $x$ is the so-called $z$-score in the context of the standardized normal distribution function. 

```{r fig.cap="Standard normal distribution function."}
# The standard normal distribution function is: y = f(x).
x <- seq(-4, 4, length = 500) # First define x.
y <- 1 / sqrt(2 * pi) * exp(-x ^ 2 / 2) # Define y as a function of x.
# Now plot.
plot(x, y, type = "l", lwd = 2, col = "red" , ylab = "dnorm(x)")
```

The copula model, including the Gaussian, considers that this performance measure $x$ in the horizontal axis is related with the firm's probability of default (from 0 to 1, or 0% to 100%). Graphically, the probability of default is represented by the area under the curve at the left of $x$. Low values of $x$ accumulate small probabilities whereas high values of $x$ accumulate high probabilities.

See the following example. If $x=3$ (bad performance of a firm), then the firm's probability of default in math notation is $N(x)$, or $N(3)$, or $\texttt{pnorm}(3)=0.998$ in R code, which is very high as it is close to 1. On the other hand, if $x=-3$ (good performance), then the probability of default $N(-3)$ or $\texttt{pnorm}(-3)=0.001349898$ is very low as it is close to 0. Here, good performance is associated with negative values of $x$ as the accumulated probability (default probability) is low. In the same way, bad performance is related with positive $x$ values. 

Then the \texttt{pnorm} R function allows us to transform $x$ into a probability of default. Transform probabilities into $x$ is also possible as the function \texttt{qnorm} is the inverse of \texttt{pnorm}. We can easily demonstrate this by retrieving the $x$ value given the probability. See for example: $N^{-1}(0.001349898)=-3$, or in R code: $\texttt{qnorm}(0.001349898)=-3$.

The function \texttt{dnorm} is relevant when we implement a graphical approach because it represents how frequent (or how likely) these values are given the standard normal distribution, so in both extreme values of $x$ the value of \texttt{dnorm} is low. Here, extreme values of $x$ can be represented by $-4$ and $4$. This function delivers the height of the standard normal distribution, $\texttt{dnorm}(4)=0.0001338302$, and $\texttt{dnorm}(-4)=0.0001338302$. Given that the standard normal function is symmetrical, we have that in general: $\texttt{dnorm}(-x)=\texttt{dnorm}(x)$. The letter $d$ in \texttt{dnorm} stands for density and it is maximum at $x=0$. When plotting the density values we get the bell-shaped normal curve. 

See how these \texttt{pnorm}, \texttt{qnorm}, \texttt{dnorm} R functions work and relate:

```{r}
# Here, x has 11 values only.
x <- c(-Inf, -4:4, Inf) # vector of x values to evaluate functions.
ans <- data.frame(x, dnorm(x), pnorm(x), qnorm(pnorm(x)))
colnames(ans) <- c("x", "dnorm(x)=height", "pnorm(x)=pd", "qnorm(pd)=x")
kable(ans, caption = "Review of normal distribution functions.", digits = 5)
```

Let's demonstrate that \texttt{dnorm} is maximum at $x=0$.

```{r}
x[which.max(ans$`dnorm(x)=height`)]
```

You can type for example \texttt{?dnorm} in the RStudio console to see more details about this (and other) functions.

Note that the only distribution function that we are currently analyzing is the standard normal (Gaussian) distribution function. A standard normal distribution function is the one that has mean 0 and variance 1. There are other copula models that assume other kinds of distributions. These other kinds of copulas are useful when we are interested in modeling cases in which extreme values are more likely to happen compared with the standard normal distribution. Understanding Gaussian copulas allows you to understand other more elaborated copulas.

The variable $D$ below represents the density function for the normal distribution. As we said before, the density is basically how frequent is a given value of $x$ in a normal distribution, so it helps to draw the typical bell-shape of the normal distribution function. Finally, $P$ is the cumulative probability function of the normal distribution, it is equivalent to the function $N(\cdot)$ in Merton's model. 

Now, let's apply the \texttt{dnorm} and \texttt{pnorm} functions for all possible values of $x$, not only in a few (11) as we did before. This will allow us to characterize a normal distribution function graphically for all possible $x$ values. To do this, we simply create a new $x$ that has 8,001 values, that would be enough. 

```{r}
# Now, x has 8001 values, this is ok to do some continuous plots.
x.theo <- seq(-4, 4, 0.001)
D <- dnorm(x.theo) 
P <- pnorm(x.theo)
```

The red area represents the probability of default. This representation requires to calculate this area, and this can be easily done by the $N(\cdot)$ function. Graphically:

```{r fig.cap="The red area represents the probability of default."}
plot(x.theo, D, type = "l", lwd = 2, 
     ylab = "Density function: dnorm(x)", xlab = "x")
polygon(c(x.theo[x.theo < 0], 0), c(D[x.theo < 0], 0), col = "red")
legend("topleft", legend = c("Here, the red area is the 
middle of the bell-shaped curve.
Smaller area means lower
probability of default.

The probability of default
at x=0 is N(0)=50%

pnorm(0)=0.5
qnorm(0.5)=0"),
bg = "white", bty = "n", cex = 0.7)
abline(v = 0, col = "black")
abline(h = dnorm(0), lty = 2)
```

A good performing firm, with a $x=-2$, imply a low probability of default.

```{r fig.cap="Low probability of default."}
plot(x.theo, D, type = "l", lwd = 2, ylab = "Density function: dnorm(x)",
     xlab = "x")
polygon(c(x.theo[x.theo < -2], -2), c(D[x.theo < -2], 0), col = "red")

legend("topright", legend = c("Smaller area means
lower probability of default.

The probability of default
at x=-2 is N(-2)=2.275%

pnorm(-2)=0.02275013
qnorm(0.02275013)=-2"),
bg = "white", bty = "n", cex = 0.7)
abline(v = -2, col = "black")
```

And this is how a bankrupt firm looks like:

```{r fig.cap="Imminent default."}
plot(x.theo, D, type = "l", lwd = 2, ylab = "Density function: dnorm(x)",
     xlab = "x")
polygon(c(x.theo[x.theo < 4], 4), c(D[x.theo < 4], 0), col = "red")

legend("topleft", legend = c("The probability of default
at x=4 is N(4)=99.996%

pnorm(4)=0.9999683
qnorm(0.9999683)=4"),
bg = "white", bty = "n", cex = 0.8)
abline(v = 4, col = "black")
```

Instead of a density function $D$, we can plot the cumulative probability distribution $P$. Now, we do not need the $N(\cdot)$ function as the vertical axis represents the probability of default.

```{r fig.cap="The higher the x, the higher the probability of default."}
plot(x.theo, P, type = "l", lwd = 2,
     ylab = "Cumulative probability function: pnorm(x)", xlab = "x")
abline(h = 0.5, lty = 2)
abline(v = 0, lty = 2)
```

Next section requires a good understanding of the example 24.6 in Hull's textbook.

## Introduction to example 24.6.

Here, we start analyzing Hull's example 24.6. We do not analyze the 10 firms yet as in the original example. We first make some sense about the data, the relevant analysis, the logic, and the model basics before dealing with the full features in example 24.6.

Recall that, \texttt{pnorm} leads to a probability, whereas \texttt{qnorm} leads to a value of $x$. Here are some examples taken directly from the textbook example, where the cumulative probabilities of default of 1%, 3%, 6%, 10% and 15% are taken as given for the maturities of 1, 2, 3, 4 and 5 years respectively. To implement the model, we take this information to derive the corresponding $x$ values:

```{r}
pd <- c(0.01, 0.03, 0.06, 0.1, 0.15) # Probabilities of default per year.
x.y <- qnorm(pd) # Transform probabilities of default into x values.
ans <- data.frame(1:5, pd, x.y) # Gather the results.
colnames(ans) <- c("year", "pd", "x (values given in Hull)")
kable(ans, caption = "Main parameters.")
```

Note that the example assumes that the probability of default increases as we consider a longer maturity (from 1 year to 5 years). Probabilities above are cumulative, so they go from year 0 to year 1, from year 0 to year 2 and so on. To calculate the probability of default during a specific year we need to calculate the differences. In particular:

```{r}
ans <- data.frame(diff(pd))
colnames(ans) <- c("PD")
rownames(ans) <- c("x.y1 to x.y2", "x.y2 to x.y3", 
                   "x.y3 to x.y4", "x.y4 to x.y5")
kable(ans, caption = "PD at specific years.")
```

This is how we can illustrate the case of a probability of default of 15% in 5 years. The green area represents the 15% of the whole area below the bell-shaped curve.

```{r fig.cap="Gaussian density distribution function."}
plot(x.theo, D, type = "l", col = 'black', lwd = 3, ylim = c(0, 0.4),
     xlab = "This could represent a firm's performance measure.
The higher, the worst performance as it accumulates more prob. of default.",
     ylab = "Density: dnorm(x)")
abline(h = 0, lty = 2)
polygon(c(x.theo[x.theo < x.y[5]], x.y[5]), 
        c(D[x.theo < x.y[5]], 0), col = "green")

legend("topright", legend=c(
"pd(5years)=15%

The left green area
represents 15% of the
whole bell-shape.

N^-1(0.15)=-1.036433
N(-1.036433)=0.15"),
bg = "white", pch = 19, cex = 0.8, bty = "n", col = "green")
```

A complementary view is the cumulative probability function. Let's illustrate the same case: a probability of default of 15% in 5 years. In this case we do not need to calculate the area since the y-axis already represents the probability.


```{r fig.cap="Gaussian probability distribution function."}
plot(x.theo, P, type = "l", col = 'black', lwd = 3, ylim = c(0, 1),
     xlab = "This could represent a firm's performance measure.
The higher, the worst performance as it accumulates more prob. of default.",
     ylab = "N(x) is a cumulative probability")
abline(h = 0, lty = 2)
abline(h = 1, lty = 2)
lines(seq(-5, x.y[5], length.out = 2), rep(pnorm(x.y[5]), 2), 
      col = "green", lwd = 3)
lines(rep(x.y[5], 2), seq(0, pnorm(x.y[5]), length.out = 2), 
      col = "green", lwd = 3)
points(x.y[5], 0.15, pch = 19, col = "green", cex = 2)
legend("right", legend=c(
"pd(5years)=15%

N^-1(0.15)=-1.036433
N(-1.036433)=0.15"),
bg = "white", pch = 19, cex = 1, bty = "n", col = "green")
```

A closer view to the figure above to see the 3-year and 5-year cases:

```{r fig.cap="Gaussian probability distribution function: Zoom version."}
plot(x.theo, P, type = "l", col = 'black', lwd = 5, ylim = c(0, 0.22), 
     xlim = c(-4, 0), ylab = "N(x) is a cumulative probability",
     xlab = "This could represent a firm's performance measure.
The higher, the worst performance as it accumulates more prob. of default.")
abline(h = 0, lty = 2)
abline(h = 1, lty = 2)
lines(seq(-5, x.y[3], length.out= 2), rep(pnorm(x.y[3]), 2), 
      col = "purple", lwd = 3, lty = 2)
lines(rep(x.y[3], 2), seq(0, pnorm(x.y[3]), length.out = 2), 
      col = "purple", lwd = 3, lty = 2)
lines(seq(-5, x.y[5], length.out = 2), rep(pnorm(x.y[5]), 2), 
      col = "green", lwd = 3, lty = 2)
lines(rep(x.y[5], 2), seq(0, pnorm(x.y[5]), length.out = 2), 
      col = "green", lwd = 3, lty = 2)
points(x.y[3], 0.06, pch = 19, col = "purple", cex = 2)
points(x.y[5], 0.15, pch = 19, col = "green", cex = 2)
legend("topleft", legend=c("pd(5years)=15%: N(-1.036433)=0.15",
                           "pd(3years)=6%: N(-1.554774)=0.06"),
pch = 19, col = c("green", "purple"), bg = "white", cex = 1, bty = "n")
```

The Gaussian copula approach is a method that takes the Gaussian distribution function to match the credit risk profile of firms.

## One firm.

Now let's use a simulation approach instead of a theoretical approach. This is, instead of generating continuous values of $x$ from $-4$ to $4$, we simulate many $x$ values (10,000 in this case) that follow a standard normal distribution function using the \texttt{rnorm} function. The simulation approach is useful especially when we are interested in replicating what happens in real-life situations because we can replicate the observed distribution many times and this facilitates the analysis. In other words, \texttt{x.theo} was used before to characterize a *perfect* normal distribution. Now, we incorporate \texttt{x.sim} that behaves as a normal distribution. These are now simulated values that follow a normal distribution, this means that we allow for some error or deviation with respect to the *perfect* normal distribution analyzed before.

Note that the probabilities of default per maturity in the simulated approach are close to the values of the previous section. In particular, $0.01$ is equivalent to $0.0102$, and $0.03$ is equivalent to $0.0307$. They do not match exactly simply because we are comparing theoretical versus simulated probabilities.

```{r}
N <- 10000 # Number of simulated values.
set.seed(130575) # Reproducibility.
x.sim <- rnorm(N, 0, 1) # Simulation.
```

```{r}
# Function to calculate proportions that we understand as probabilities.
prop <- function(x) {
  ans <- length(x.sim[x.sim <= x]) / N
  }
pd.sim <- mapply(prop, x.y) # Apply the function.
ans <- data.frame(x.y, pd, pd.sim)
colnames(ans) <- c("x", "pd.theo", "pd.sim")
rownames(ans) <- c("y1", "y2", "y3", "y4", "y5")
kable(ans, caption = "Theoretic versus simulated probabilities of default.")
```

Let's view the results of the simulated approach. First in a histogram.

```{r fig.cap="Simulated Gaussian probability distribution function. Somewhat different with respect to the theoretical."}
# Some parameters we need to plot.
L <- c(-4, 4) # axis limits.
colors2 <- c("blue", "red", "purple", "pink", "green")
legend2 = c("pd(1year)=1.02%: x<=-2.326348", 
"pd(2years)=3.07%: x<=-1.880794", "pd(3years)=6.29%: x<=-1.554774",
"pd(4years)=10.08%: x<=-1.281552", "pd(5years)=14.8%: x<=-1.036433")
# The histogram.
hist(x.sim, 500, xlim = L, ylim = c(0, 100), main = NULL, xlab = "x
This could represent a firm's simulated performance measure")
abline(h = 0, lty = 2)
abline(v = x.y[1], lwd = 3, col = "blue")
abline(v = x.y[2], lwd = 3, col = "red")
abline(v = x.y[3], lwd = 3, col = "purple")
abline(v = x.y[4], lwd = 3, col = "pink")
abline(v = x.y[5], lwd = 3, col = "green")
legend("topright", legend = legend2, bg = "white", 
       text.col = colors2, cex = 0.7)
```

The area at the left hand side of each colored line represents the cumulative probability of default just as we explained before. In the same way, the area between two colored lines represents the probability of default in a specific period of time. 

Now let's see all the simulated data at once.

```{r fig.cap="An alternative view."}
plot(x.sim, ylab = "One firm performance", pch = ".", 
     ylim = c(-4, 7),
     xlab = "10,000 simulated performance data")
abline(h = x.y[1], lwd = 2, col = "blue")
abline(h = x.y[2], lwd = 2, col = "red")
abline(h = x.y[3], lwd = 2, col = "purple")
abline(h = x.y[4], lwd = 2, col = "pink")
abline(h = x.y[5], lwd = 2, col = "green")
abline(v = 0, lty = 2)
abline(v = 10000, lty = 2)
legend("topright", legend = legend2, bg = "white", 
       text.col = colors2, cex = 0.8)
```

We normally conduct a simulation approach because we might adapt the distribution function parameters to match what we see in the real life situations. The simulation approach allows us to have such flexibility. 

## Two firms.

Now consider the case in which we have two firms instead of one. The main difference now is that instead of simulating one firm we need two. Moreover, each firm follows a standard normal distribution function and both of them are correlated by a given correlation value so the firms are not independent. If they are not independent, then what happens to one firm has some impact on what happens to the other. In this case, we assume 0.2 as a correlation value. The case of two firms is not the one presented in Hull's example but it can help us to visualize how the Gaussian copula approach works in a two-dimension plot.

The simulation of both firms' performance measures is \texttt{x2}.

```{r}
library(MASS)
m <- 2 # number of firms
n <- 10000 # number of simulations
rho_pos02 <- 0.2 # correlation
corr_pos02 <- matrix(rep(rho_pos02, m * m), m, m) # correlation matrix
diag(corr_pos02) <- 1
set.seed(130575)
x2 <- mvrnorm(n, mu = rep(0, m), Sigma = corr_pos02)
x2 <- data.frame(x2)
colnames(x2) <- c("Firm1", "Firm2")
```

The matrix \texttt{x2} length is 10,000 for each firm. In other words, we have 10,000 observations of the performance measure or $z$-score for two firms that are related. This matrix is big, but we can visualize the header (the first six observations).

```{r}
kable(head(x2), caption = "Firm's performance.", row.names = TRUE)
```

Remember the cumulative probabilities of default are 1%, 3%, 6%, 10% and 15% for the maturities of 1, 2, 3, 4 and 5 years respectively. How do we extract those cases in which both firms will default in 5 years? In rows 15, 53, 61 and so on both firms default at the same time. Note that in all cases the $x$ values are indeed below -1.036433.

```{r}
# These names are going to be useful later.
n.year <- c("year 1", "year 2", "year 3", "year 4", "year 5")
n.pd <- c("pd.y1", "pd.y2", "pd.y3", "pd.y4", "pd.y5")
n.f <- c("Firm1", "Firm2", "Firm1 default?", "Firm2 default?")

# Function to calculate cases in which firms default and probabilities.
fun.X <- function(x) {
  both <- x2[x2$Firm2 < x & x2$Firm1 < x, ] # both default.
  atleast1 <- x2[x2$Firm2 < x | x2$Firm1 < x, ] # at least one firm default.
  onlyfirm1 <- x2[x2$Firm2 > x & x2$Firm1 < x, ] # only firm 1 default.
  onlyfirm2 <- x2[x2$Firm1 > x & x2$Firm2 < x, ] # only firm 2 default.
  onlyone <- x2[(x2$Firm2 < x & x2$Firm1 > x | # only one firm default.
                   x2$Firm2 > x & x2$Firm1 < x),]
  none <- x2[x2$Firm2 > x & x2$Firm1 > x, ] # no firm default.
# Gather results and probabilities in a list.
ans <- list(both = both, atleast1 = atleast1, onlyfirm1 = onlyfirm1,
  onlyfirm2 = onlyfirm2, onlyone = onlyone, none = none,
  both.pd = (nrow(both) / n), atleast1.pd = (nrow(atleast1) / n),
  onlyfirm1.pd = (nrow(onlyfirm1) / n), onlyfirm2.pd = (nrow(onlyfirm2)/ n),
  onlyone.pd = (nrow(onlyone) / n), none.pd = (nrow(none) / n))
}
# X has all the relevant results for x2.
X <- mapply(fun.X, x.y)
```

See the cases in which both firms default in 5 years.

```{r}
# Extract "both" cases, year 5.
both <- data.frame(X[["both", 5]], X[["both", 5]] < x.y[5])
colnames(both) <- n.f
kable(head(both), caption = "Cases in which both firms default in 5 years.", 
      row.names = TRUE)
```
In total, we have 343 cases in which both firms default at the same time. The first case is number 15, the second 53, the third 61 and so on. It is easy to know the total cases if we count the number of rows.

How do we extract those cases in which at least one firm will default in 5 years? This is, only firm 1, only firm 2 and even both at the same time. This is a less strict condition so we would expect to have more cases to match this new criteria compared with \texttt{both}. Note that in all cases at least one one firm is indeed below -1.036433. In row 1, 2 and 10 firm 2 defaults. In row 15 both firms default. In row 18 and 20 firm 1 and firm 2 default respectively.


```{r}
atleast1 <- data.frame(X[["atleast1", 5]], X[["atleast1", 5]] < x.y[5])
colnames(atleast1) <- n.f
kable(head(atleast1), caption = "Cases in which at least one firm default.", 
      row.names = TRUE)
```

Now we have 2,660 cases. Considerably more as the | restriction is less strict than the &. Let's see the cases in which only firm 1 defaults.

```{r}
onlyfirm1 <- data.frame(X[["onlyfirm1", 5]], X[["onlyfirm1", 5]] < x.y[5])
colnames(onlyfirm1) <- n.f
kable(head(onlyfirm1), caption = "Cases in which only firm 1 default.")
```

Only firm 2 defaults. 

```{r}
onlyfirm2 <- data.frame(X[["onlyfirm2", 5]], X[["onlyfirm2", 5]] < x.y[5])
colnames(onlyfirm2) <- n.f
kable(head(onlyfirm2), caption = "Cases in which only firm 2 default.")
```

Only one firm default at year 5.

```{r}
onlyone <- data.frame(X[["onlyone", 5]], X[["onlyone", 5]] < x.y[5])
colnames(onlyone) <- n.f
kable(head(onlyone), caption = "Cases in which only one firm default.")
```

Lastly, when no firm defaults.

```{r}
none <- data.frame(X[["none", 5]], X[["none", 5]] < x.y[5])
colnames(none) <- n.f
kable(head(none), caption = "Cases in which no firm default.")
```

Finally, probabilities.

```{r}
both.pd <- t(data.frame(X["both.pd",]))
atleast1.pd <- t(data.frame(X["atleast1.pd",]))
onlyfirm1.pd <- t(data.frame(X["onlyfirm1.pd",]))
onlyfirm2.pd <- t(data.frame(X["onlyfirm2.pd",]))
onlyone.pd <- t(data.frame(X["onlyone.pd",]))
none.pd <- t(data.frame(X["none.pd",]))
ans <- data.frame(both.pd, atleast1.pd, onlyfirm1.pd, onlyfirm2.pd, 
                 onlyone.pd, none.pd)
rownames(ans) <- n.year
kable(ans, caption = "Probabilities of default.")
```

So interesting. 

Note that 15% is the theoretical probability that one firm will default in 5 years. Here, this 15% is 12.1% for firm 1 and 11.07% for firm 2 when the data is simulated. 

We can even perform a nice test to see that everything is alright. For example, this equation must hold: \texttt{onlyFirm1+onlyFirm2=onlyonefirm}. Substituting for the year 5: $0.121+0.1107=0.2317$. As you can see, everything is alright. This equation must hold as well: \texttt{atleast1-both=onlyonefirm}. Substituting for the year 5: $0.266-0.0343=0.2317$. As you can see, everything is alright.

Let's visualize all 10,000 cases. Each dot represents a couple of Firm1 and Firm2 $x$ values and the dotted lines the threshold that represents the probability of default in 5 years.

```{r fig.cap="All 10,000 simulated cases."}
par(pty = "s") # Figures are shown in a perfect square (not a rectangle).
plot(x2, pch = ".", cex = 0.8) 
points(mean(x2[,1]), mean(x2[,2]), col = "red", pch = 19, cex = 1)
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("bottomright", legend = c(paste(nrow(x2))), bty = "n")
```

These 10,000 observations are highly concentrated around the mean which is very close to zero $(-0.01883215,-0.0147721)$, note the red point. This can be also easily seen in the following density plot.

```{r fig.cap="All 10,000 simulated cases: A density view."}
library(ggplot2)
library(dplyr)
library(viridis)
df <- tibble(x2)
par(pty = "s")
ggplot(df, aes(x = x2$Firm1, y = x2$Firm2)) +
  stat_density2d(aes(fill = ..density..), contour = F, 
                 geom = 'tile') +
  scale_fill_viridis()+
  coord_fixed()
```


```{r eval=FALSE, include=FALSE}
library(rayshader)
df <- tibble(x2)
par(pty = "s")
rayplot <- ggplot(df, aes(x = x2$Firm1, y = x2$Firm2)) +
  stat_density2d(aes(fill = ..density..), contour = F, 
                 geom = 'tile') +
  scale_fill_viridis()+
  #scale_fill_viridis_c(option = "A")+
  coord_fixed() +
  theme(legend.position = "none")
plot_gg(rayplot, width = 4, height = 4, scale = 400, zoom = 0.7,
        multicore = TRUE)
# Movie and picture.
#render_movie("p25.mp4", frames = 460)
#render_snapshot("p25.png", clear = TRUE)

```

```{r eval=FALSE, include=FALSE}
df <- tibble(x2)
par(pty = "s")
rayplot <- ggplot(df, aes(x = x2$Firm1, y = x2$Firm2)) +
  stat_density2d(aes(fill = ..density..), contour = F, 
                 geom = 'tile') +
    geom_hex(bins = 35, size = 0.5, color = "black") +
  #scale_fill_viridis()+
  #scale_fill_viridis_c(option = "A")+
    scale_fill_viridis_c(option = "C")+
  coord_fixed() +
  theme(legend.position = "none")

plot_gg(rayplot, width = 4, height = 4, scale = 400, zoom = 0.7,
        multicore = TRUE)

```

We can visualize the default cases. First, the case when both firms default at year 5.

```{r fig.cap="Both firms default at year 5."}
par(pty = "s")
plot(X[["both", 5]], xlim  = L, ylim = L, pch = ".", cex = 0.8) 
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(both.pd[5]*n)), bty = "n")
```

The values within the plot represent the number of cases. Here, we have 343 times (out of 10,000) in which both firms default at the same time in 5 years. Note that this is a cumulative probability of default.

Now, the case in which at least one firm defaults in 5 years.

```{r fig.cap="At least one firm defaults in 5 years."}
par(pty = "s")
plot(X[["atleast1", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8)
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(atleast1.pd[5]*n)), bty = "n")
```

Only firm 1 defaults in 5 years.

```{r fig.cap="Only firm 1 defaults in 5 years."}
par(pty = "s")
plot(X[["onlyfirm1", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8)
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(onlyfirm1.pd[5]*n)), bty = "n")
```

Only firm 2 defaults in 5 years.

```{r fig.cap="Only firm 2 defaults in 5 years."}
par(pty = "s")
plot(X[["onlyfirm2", 5]], xlim = L, ylim=L, pch = ".", cex = 0.8)
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(onlyfirm2.pd[5]*n)), bty = "n")
```

This is the case in which only one firm defaults.

```{r fig.cap="Only one firm defaults in 5 years."}
par(pty = "s")
plot(X[["onlyone", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8)
abline(h = x.y[5], lty =2)
abline(v = x.y[5], lty =2)
legend("topright", legend = c(paste(onlyone.pd[5]*n)), bty = "n")
```

This is the case in which no one firm defaults.

```{r fig.cap="No firm defaults in 5 years."}
par(pty = "s")
plot(X[["none", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8)
abline(h = x.y[5], lty =2)
abline(v = x.y[5], lty =2)
legend("topright", legend = c(paste(none.pd[5]*n)), bty = "n")
```

It is a good idea to summarize all previous plots in one.

```{r fig.cap="Which firm defaults at year 5?"}
# none
par(mfrow = c(2, 3), oma = c(0, 0, 2, 0))
par(pty = "s")
plot(X[["none", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8, 
     main = "None.") 
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("bottomright", legend = c(paste(none.pd[5]*n)), bty = "n")
# both
par(pty = "s")
plot(X[["both", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8, 
     main = "Both.") 
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(both.pd[5]*n)), bty = "n")
# atleast1
par(pty = "s")
plot(X[["atleast1", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8, 
     main = "At least one.")
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(atleast1.pd[5]*n)), bty = "n")
# onlyfirm1
par(pty = "s")
plot(X[["onlyfirm1", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8, 
     main = "Only Firm1.")
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(onlyfirm1.pd[5]*n)), bty = "n")
# onlyfirm2
par(pty = "s")
plot(X[["onlyfirm2", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8, 
     main = "Only Firm2.")
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("topright", legend = c(paste(onlyfirm2.pd[5]*n)), bty = "n")
# onlyone
par(pty = "s")
plot(X[["onlyone", 5]], xlim = L, ylim = L, pch = ".", cex = 0.8, 
     main = "Only one.")
abline(h = x.y[5], lty = 2)
abline(v = x.y[5], lty = 2)
legend("topright", legend = c(paste(onlyone.pd[5]*n)), bty = "n")
```

It is interesting to compare two different correlation values. Here, we compare 0 versus 0.2.

```{r}
m <- 2 # number of firms
n <- 10000 # number of simulations
rho_pos02 <- 0 # correlation
corr_pos02 <- matrix(rep(rho_pos02, m * m), m, m) # correlation matrix
diag(corr_pos02) <- 1
set.seed(130575)
X0 <- mvrnorm(n, mu = rep(0, m), Sigma = corr_pos02)
X0 <- data.frame(X0)
colnames(X0) <- c("Firm1", "Firm2")
```

Just as before, we create a function, evaluate it, and store results in \texttt{X0}.

```{r}
fun.X0 <- function(x) {
  both <- X0[X0$Firm2 < x & X0$Firm1 < x, ]
  atleast1 <- X0[X0$Firm2 < x | X0$Firm1 < x, ]
  onlyfirm1 <- X0[X0$Firm2 > x & X0$Firm1 < x, ]
  onlyfirm2 <- X0[X0$Firm1 > x & X0$Firm2 < x, ]
  onlyone <- X0[(X0$Firm2 < x & X0$Firm1 > x | 
                   X0$Firm2 > x & X0$Firm1 < x),]
  none <- X0[X0$Firm2 > x & X0$Firm1 > x, ]
  
ans <- list(both = both, atleast1 = atleast1, onlyfirm1 = onlyfirm1,
  onlyfirm2 = onlyfirm2, onlyone = onlyone, none = none,
  both.pd = (nrow(both) / n), atleast1.pd = (nrow(atleast1) / n),
  onlyfirm1.pd = (nrow(onlyfirm1) / n), onlyfirm2.pd = (nrow(onlyfirm2) /n),
  onlyone.pd = (nrow(onlyone) / n), none.pd = (nrow(none) / n)) }

X0 <- mapply(fun.X0, x.y)

none.pd0 <- data.frame(X0["none.pd",]) * n
both.pd0 <- data.frame(X0["both.pd",]) * n

onlyfirm1.pd0 <- data.frame(X0["onlyfirm1.pd",]) * n
onlyfirm2.pd0 <- data.frame(X0["onlyfirm2.pd",]) * n
```

A graphical analysis shows that in the case of 0.2 it is more likely that both firms default at the same time, and it is less likely that any firm default at the same time.


```{r fig.cap="Cases per quadrant. Dotted lines corresponds to year 5."}
par(mfrow=c(1, 2), oma = c(0, 0, 2, 0))
par(pty = "s")
plot(X[["none", 5]], pch = ".", xlim = L, ylim = L, 
     cex = 0.8, main = "Correlation=0.2") 
points(X[["both", 5]], pch = ".", col = "red")
points(X[["onlyfirm1", 5]], pch = ".", col = "purple")
points(X[["onlyfirm2", 5]], pch = ".", col = "blue")
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("bottomleft", legend = c(paste(both.pd[5]*n)), bty = "n")
legend("topright", legend = c(paste(none.pd[5]*n)), bty = "n")
legend("topleft", legend = c(paste(onlyfirm1.pd[5]*n)), bty = "n")
legend("bottomright", legend = c(paste(onlyfirm2.pd[5]*n)), bty = "n")
par(pty = "s")
plot(X0[["none", 5]], pch = ".", xlim = L, ylim = L,
     cex = 0.8, main = "Correlation=0.") 
points(X0[["both", 5]], pch = ".", col = "red")
points(X0[["onlyfirm1", 5]], pch = ".", col = "purple")
points(X0[["onlyfirm2", 5]], pch = ".", col = "blue")
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("bottomleft", legend = c(paste(both.pd0[5])), bty = "n")
legend("topright", legend = c(paste(none.pd0[5])), bty = "n")
legend("topleft", legend = c(paste(onlyfirm1.pd0[5])), bty = "n")
legend("bottomright", legend = c(paste(onlyfirm2.pd0[5])), bty = "n")
par(pty = "s")
```
Very interesting indeed. 

```{r}
m <- 2 # number of firms
n <- 10000 # number of simulations
rho_pos00 <- 0 # correlation
rho_pos02 <- 0.2
corr_pos00 <- matrix(rep(rho_pos00, m * m), m, m) # correlation matrix
corr_pos02 <- matrix(rep(rho_pos02, m * m), m, m) # correlation matrix
diag(corr_pos00) <- 1
diag(corr_pos02) <- 1
set.seed(130575)
X00 <- mvrnorm(n, mu = rep(0, m), Sigma = corr_pos00)
set.seed(130575)
X02 <- mvrnorm(n, mu = rep(0, m), Sigma = corr_pos02)
X00.02 <- data.frame(rbind(X00, X02))
colnames(X00.02) <- c("Firm1", "Firm2")

n=20000
fun.X0 <- function(x) {
  both <- X00.02[X00.02$Firm2 < x & X00.02$Firm1 < x, ]
  atleast1 <- X00.02[X00.02$Firm2 < x | X00.02$Firm1 < x, ]
  onlyfirm1 <- X00.02[X00.02$Firm2 > x & X00.02$Firm1 < x, ]
  onlyfirm2 <- X00.02[X00.02$Firm1 > x & X00.02$Firm2 < x, ]
  onlyone <- X00.02[(X00.02$Firm2 < x & X00.02$Firm1 > x | 
                   X00.02$Firm2 > x & X00.02$Firm1 < x),]
  none <- X00.02[X00.02$Firm2 > x & X00.02$Firm1 > x, ]
  
ans <- list(both = both, atleast1 = atleast1, onlyfirm1 = onlyfirm1,
  onlyfirm2 = onlyfirm2, onlyone = onlyone, none = none,
  both.pd = (nrow(both) / n), atleast1.pd = (nrow(atleast1) / n),
  onlyfirm1.pd = (nrow(onlyfirm1) / n), onlyfirm2.pd = (nrow(onlyfirm2) /n),
  onlyone.pd = (nrow(onlyone) / n), none.pd = (nrow(none) / n)) }

X00.02 <- mapply(fun.X0, x.y)

none.pd0002 <- data.frame(X00.02["none.pd",]) * n
both.pd0002 <- data.frame(X00.02["both.pd",]) * n

onlyfirm1.pd0002 <- data.frame(X00.02["onlyfirm1.pd",]) * n
onlyfirm2.pd0002 <- data.frame(X00.02["onlyfirm2.pd",]) * n

```


```{r}
par(pty = "s")
plot(X00.02[["none", 5]], pch = ".", xlim = L, ylim = L, 
     cex = 0.8, main = "Correlation=0.2") 
points(X00.02[["both", 5]], pch = ".", col = "red")
points(X00.02[["onlyfirm1", 5]], pch = ".", col = "purple")
points(X00.02[["onlyfirm2", 5]], pch = ".", col = "blue")
abline(v = x.y[5], lty = 2)
abline(h = x.y[5], lty = 2)
legend("bottomleft", legend = c(paste(both.pd0002[5]/n*100)), bty = "n")
legend("topright", legend = c(paste(none.pd0002[5]/n*100)), bty = "n")
legend("topleft", legend = c(paste(onlyfirm1.pd0002[5]/n*100)), bty = "n")
legend("bottomright", legend = c(paste(onlyfirm2.pd0002[5]/n*100)), bty = "n")

```


## Ten firms.

The original Hull's example proposes a 10 firm case and here we implement this example following a simulation approach. First, we need a $10\times10$ correlation matrix to produce the new $x$ values using a random multi-variate distribution algorithm. According to Hull's example the copula default correlations between each pair of companies is 0.2. The code below has the option to vary the default correlation given a uniform random distribution.

The new $10\times10$ correlation matrix is then:

```{r}
# Create the correlation matrix.
m <- 10 # number of firms.
n <- 1000000 # number of simulations per firm.
x <- matrix(rep(0.2, m * m), m, m) 
ind <- lower.tri(x) 
x[ind] <- t(x)[ind] 
diag(x) = 1
kable(x, caption = "Correlation matrix 0.2.")
```
Now, we can simulate the multivariate normal distribution. The variable \texttt{X10} length is 1,000,000 for each firm. This is big, but we can visualize the header of this variable. We choose a higher number of simulations because now we need 10 firms to meet a single constraint.

```{r}
# Create the simulated cases.
set.seed(130575) # Reproducibility.
X10 <- mvrnorm(n, mu = rep(0, m), Sigma = x)
X10 <- data.frame(X10) # 10,000,000 observations.
kable(head(X10), caption = "10 firms' performance, 1,000,000 simulations.",
      digits = 3)
```

How do we extract those cases in which all 10 firms will default in 5 years (at the same time)? Here are the first 6 of those cases. Note that in all cases the \texttt{X10} values are lower than $-1.036433$.

```{r}
# Given that we have 10 firms, it is easier to use filter_all function.
# Although probably this could be simplified even further.
y5.all.2 <- filter_all(X10, all_vars(. < x.y[5]))
y4.all.2 <- filter_all(X10, all_vars(. < x.y[4])) 
y3.all.2 <- filter_all(X10, all_vars(. < x.y[3])) 
y2.all.2 <- filter_all(X10, all_vars(. < x.y[2])) 
y1.all.2 <- filter_all(X10, all_vars(. < x.y[1])) 
```

Let's analyze the case of 5 years.

```{r}
kable(head(y5.all.2), 
      caption = "Cases in which all 10 firms default in five years.", 
      digits = 3)
```

```{r}
kable((head(y5.all.2) < x.y[5]), caption = "Check if all of them default.")
```

How many cases are there?

```{r}
nrow(y5.all.2)
```
Which are those 72 cases?

```{r}
# Here, I compare only firm 1 as if firm 1 defaults, then the rest default.
kable(matrix(which(X10[,1] %in% y5.all.2[,1]), 6, 9), 
      caption = "Which of the 1,000,000 cases represent a default of all 
      firms in five years?")
```

In total, we only have 72 cases. This is, the 10 firms will default at the same time in 5 years in 72 out of 1,000,000 total cases. For the rest of the years, the cases are less frequent, in fact we have zero cases for year 1, 2 and 3.

How do we extract those cases in which at least one of the 10 firms will default? 

```{r}
y5.any.2 <- filter_all(X10, any_vars(. < x.y[5]))
y4.any.2 <- filter_all(X10, any_vars(. < x.y[4]))
y3.any.2 <- filter_all(X10, any_vars(. < x.y[3]))
y2.any.2 <- filter_all(X10, any_vars(. < x.y[2]))
y1.any.2 <- filter_all(X10, any_vars(. < x.y[1]))
```

Here are the first 6 cases for the 5-year default.

```{r}
kable(head(y5.any.2), caption = "At least one firm default in five years.",
      digits = 3)
```

```{r}
kable((head(y5.any.2) < x.y[5]), caption = "Check which one(s) default.")
```

How many cases are these?

```{r}
nrow(y5.any.2)
```

In total, we have 682,148 cases. This is, at least one of 10 firms will default in 5 years in 682,148 of 1,000,000 cases. 

And how to convert them into probabilities?

```{r}
atleastone02 <- t(data.frame(nrow(y1.any.2), nrow(y2.any.2),
                nrow(y3.any.2), nrow(y4.any.2), nrow(y5.any.2)))
all02 <- t(data.frame(nrow(y1.all.2), nrow(y2.all.2),
                nrow(y3.all.2), nrow(y4.all.2), nrow(y5.all.2)))
res02 <- data.frame(all02 / n, atleastone02 / n)
rownames(res02) <- n.year
colnames(res02) <- c("All firms", "At least one")
kable(res02, caption = "Probabilities of default (10 firms, corr=0.2).")
```

Let's see the difference when we assume a different correlation matrix. This case, the correlation vary randomly between 0.45 and 0.65.

```{r}
m <- 10 # number of firms
n <- 1000000 # number of simulations
set.seed(130575)
x <- matrix(runif(m * m, 0.45, 0.65), m, m) 
ind <- lower.tri(x) 
x[ind] <- t(x)[ind] 
diag(x) = 1
kable(x, caption = "Correlation between 0.45 and 0.65.", digits = 3)
```

The resulting values of \texttt{Xr} are:

```{r}
set.seed(130575)
Xr <- mvrnorm(n, mu = rep(0, m), Sigma = x)
Xr <- data.frame(Xr)
kable(head(Xr), caption = "Firms' performance.", digits = 3)
```

Extract the cases in which all ten firms default at the same time in 5 years and the cases in which either firm default at the same time in 5 years.

```{r}
Xr <- as_tibble(Xr)
# All firms.
Xr.y1.all <- filter_all(Xr, all_vars(. < x.y[1])) 
Xr.y2.all <- filter_all(Xr, all_vars(. < x.y[2])) 
Xr.y3.all <- filter_all(Xr, all_vars(. < x.y[3])) 
Xr.y4.all <- filter_all(Xr, all_vars(. < x.y[4]))
Xr.y5.all <- filter_all(Xr, all_vars(. < x.y[5])) 
# At least one firm.
Xr.y1.any <- filter_all(Xr, any_vars(. < x.y[1]))
Xr.y2.any <- filter_all(Xr, any_vars(. < x.y[2]))
Xr.y3.any <- filter_all(Xr, any_vars(. < x.y[3]))
Xr.y4.any <- filter_all(Xr, any_vars(. < x.y[4]))
Xr.y5.any <- filter_all(Xr, any_vars(. < x.y[5]))
```

All firms default in 5,935 cases out of 1,000,000.

```{r}
nrow(Xr.y5.all)
```
Let's see the first 6 cases:

```{r}
kable(head(Xr.y5.all), 
      caption = "Cases in which all 10 firms default in five years.", 
      digits = 3)
```

Verify that those cases default.

```{r}
kable((head(Xr.y5.all) < x.y[5]), caption = "Check if all of them default.")
```

At least one firm default in 497,987 out of 1,000,000.

```{r}
nrow(Xr.y5.any)
```

```{r}
kable(head(Xr.y5.any), caption = "At least one firm default in five years.",
      digits = 3)
```

```{r}
kable((head(Xr.y5.any) < x.y[5]), caption = "Check which one(s) default.")
```

And the corresponding probabilities:

```{r}
allR <- t(data.frame(nrow(Xr.y1.all), nrow(Xr.y2.all),
        nrow(Xr.y3.all), nrow(Xr.y4.all), nrow(Xr.y5.all)))
atleastoneR <- t(data.frame(nrow(Xr.y1.any), nrow(Xr.y2.any),
        nrow(Xr.y3.any), nrow(Xr.y4.any), nrow(Xr.y5.any)))
resR <- data.frame(allR / n, atleastoneR / n)
ans <- data.frame(res02, resR)
rownames(ans) <- n.year
colnames(ans) <- c("All (corr=0.2)", "At least one (corr=0.2)",
                    "All (corr=rand)", "At least one (corr=rand)")
kable(ans, caption = "Probability of default.", 
      row.names = TRUE)
```

# Credit VaR example 24.7.

This replicates Hull's example 24.7.

This is the Vasicek model (equation 24.10) in a function form. 

```{r}
CVaR <- function(exp, pd, r, c, l) {
v <- pnorm((qnorm(pd) + (c^0.5) * qnorm(l)) / (1 - c)^0.5)
VaR <- exp * v * (1 - r)
}
```

See if it works. Let's evaluate equation 24.10 at 99% and 99.9%. The 1-year 99% and 99.9% credit VaR is:

```{r}
CVaR.999 <- CVaR(100, 0.02, 0.6, 0.1, 0.999)
CVaR.99 <- CVaR(100, 0.02, 0.6, 0.1, 0.99)
CVaR.999
CVaR.99
```

Let's evaluate the model at all confidence levels (from 0 to 1) simulating 10,000 values.

```{r}
set.seed(13)
l <- runif(10000, 0, 1)
Loss <- CVaR(100, 0.02, 0.6, 0.1, l)
sim.var999 <- sort(Loss)[10000 * 0.999]
sim.var99 <- sort(Loss)[10000 * 0.99]
```

Now, visually:

```{r fig.cap="Distribution of losses."}
hist(Loss, 100, xlim = c(0, 7), xlab = "Losses in millions", main = NULL)
abline(v = CVaR.999, lty = 2, col = "red", lwd = 2)
abline(v = CVaR.99, lty = 2, col = "blue", lwd = 2)
legend("topright", legend = c("1-year 99% credit VaR is 3.294271", 
         "1-year 99.9% credit VaR is 5.129484"),
col = c("blue", "red"), lwd = 2, lty = 2, bg = "white", cex = 0.8)
```


```{r fig.cap="Distribution of losses."}
dat <- data.frame(Loss, l)
ggplot(dat, aes(x = Loss, fill = "Losses in millions")) + 
  geom_density(color = "darkblue", fill = "lightblue") +
geom_vline(aes(xintercept = CVaR.999),  
               color = "red", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = CVaR.99 ),   
               color = "blue", linetype = "dashed", size = 1)
```
Nice.

# References.

---
nocite: '@*'
...
